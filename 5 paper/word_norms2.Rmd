---
title             : "English Semantic Feature Production Norms: An Extended Database of 4,437 Concepts"
shorttitle        : "Semantic Norms"

author: 
  - name          : "Erin M. Buchanan"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "901 S. National Ave, Springfield, MO 65897"
    email         : "erinbuchanan@missouristate.edu"
  - name          : "K. D. Valentine"
    affiliation   : "2"
  - name          : "Nicholas P. Maxwell"
    affiliation   : "1"

affiliation:
  - id            : "1"
    institution   : "Missouri State University"
  - id            : "2"
    institution   : "University of Missouri"

author_note: |
  Erin M. Buchanan is an Associate Professor of Quantitative Psychology at Missouri State University. K. D. Valentine is a Ph.D. candidate at the University of Missouri. Nicholas P. Maxwell is a Masters' candidate at Missouri State University.
  
  We would like to thank Keith Hutchison and David Balota for their contributions to this project, including the funds to secure Mechanical Turk participants. 

abstract: |
  The largest limiting factor in understanding memory and language networks is often the availability of normed stimuli to use and explore in experimental studies. In this study, we expand on three previous semantic feature overlap norms to over 4,000 cue stimuli ranging from nouns, verbs, adjectives, and other parts of speech. Participants in the norming study were asked to provide feature components of each cue stimuli, which were combined with the previous research using semantic feature production procedures. In addition to expanding previous research, this project explores different semantic overlap measurements by coding each word feature listed by root and affixes to determine different strengths of feature overlap. All information is provided in a searchable database for easy access and utilization for future researchers when designing experiments. The final database of cue-target pairs was paired with the Semantic Priming Project to examine the relation of feature overlap statistics on semantic priming in tandem with other psycholinguistic variables, such as association and thematics. 
  
keywords          : "semantics, word norms, database, psycholinguistics"

bibliography      : ["r-references.bib", "wn2.bib"]

figsintext        : no
figurelist        : no
tablelist         : no
footnotelist      : no
lineno            : yes

lang              : "english"
class             : "man"
output            : papaja::apa6_pdf
replace_ampersands: yes
csl               : apa6.csl
---

```{r load_packages, include = FALSE}
library("papaja")
library(MOTE)
library(shiny) #to cite shiny
```

Semantic representations are the focus of a large area of research and are a thing.

What are semantic feature production norms

Why are they important!

Previous work 

# Method

## Participants

Participants in the newly collected stimuli were gathered from Amazon's Mechanical Turk, which is a large, diverse participant pool wherein users can complete surveys for small sums of money [@Buhrmester2011]. Answers can be screened for errors, and incorrect or incomplete surveys can be rejected or discarded without payment. Each participant was paid five cents for a survey, and they could complete multiple Human Intelligence Tasks or HITS. Each HIT included five concepts, and HITS would remain active until *n* = 30 valid survey answers were collected. HITS were usually rejected if they included copied definitions from Wikipedia, "I don't know", or writing a paragraph about the concept. These answers were discarded, as described below. Table \@ref(tab:part-table) includes the sample sizes from the new study (Mechanical Turk 2), as well as the sample sizes from the previous study, as described in @Buchanan2013. 

```{r part-table, echo=FALSE, results='asis'}
####partcipant table####
participant.table2 = matrix(NA, nrow = 5, ncol = 4)

colnames(participant.table2) = c("Institution", "Total Participants",
                         "Concepts", "Mean $N$")

participant.table2[1, ] = c("University of Mississippi", 749, 658, 67.8)
participant.table2[2, ] = c("Missouri State University", 1420, 720, 71.4)
participant.table2[3, ] = c("Montana State University", 127, 120, 63.5)
participant.table2[4, ] = c("Mechanical Turk 1", 571, 310, 60)
participant.table2[5, ] = c("Mechanical Turk 2", 198, 1914, 30)         

apa_table.latex(as.data.frame(participant.table2),
                align = c(rep("l", 1), rep("c", 3)), 
                caption = "",
                note = "",
                escape = FALSE,
                col.names = c("Institution", "Total Participants",
                         "Concepts", "Mean $N$"))
```

## Materials

The purpose of this second norming set was to expand the @Buchanan2013 norms to include all concepts from the Semantic Priming Project [@Hutchison2010]. Therefore, these concepts were the target of the project. The original concept set was selected primarily from the @Nelson2004 database, with small overlaps in the @McRae2005 and @Vinson2008 database sets for convergent validity. In the Semantic Priming Project, cue-target pairs were shown to participants to examine naming and lexical decision time priming across related and unrelated pairs. The related pairs included first associate (most common response to a cue) and other associates (second or greater common responses to cues) as their target words. The original publication of concepts included the cue words from the Semantic Priming Project, while this project expanded to include missed cue words and all target words. The addition of these concepts allowed for complete overlap between the Semantic Priming Project and the feature production norms. 

```{r calculate-frequency, include=FALSE}

dat1 = read.csv("final words 2017.csv")
sub1 = subset(dat1,
              dat1$where == "b")

####stimuli table####

#we want the number of features for each concept
#reduce down to just the translated data
sub1.reduce = sub1[ , c("cue", "translated", "pos_cue", "school_code")]
sub1.reduce = unique(sub1.reduce)
sub1.reduce$cue = droplevels(sub1.reduce$cue)

#for each cue calculate the number of features listed
nofeatures = as.data.frame(table(sub1.reduce$cue))
colnames(nofeatures)[1] = "cue"

#add in the school code and pos_cue
finalfreq = unique(merge(nofeatures, sub1.reduce[ , c("cue", "pos_cue", "school_code")], by = "cue"))

#now calculate statistics - means by school
with(finalfreq, tapply(Freq, list(pos_cue, school_code), mean))
with(finalfreq, tapply(Freq, list(pos_cue, school_code), sd))

##mean frequency per school
with(finalfreq, tapply(Freq, list(school_code), mean))
with(finalfreq, tapply(Freq, list(school_code), sd))

##mean frequency by part of speech
with(finalfreq, tapply(Freq, list(pos_cue), mean))
with(finalfreq, tapply(Freq, list(pos_cue), sd))

##total total 
mean(finalfreq$Freq); sd(finalfreq$Freq)

partofspeech = unique(dat1[ , c("cue", "pos_cue")])
POS = apa(table(partofspeech$pos_cue)/nrow(partofspeech)*100,1)
```

Concepts were labeled by part of speech using the English Lexicon Project [@Balota2007], the free association norms, and Google's define search when necessary. When labelling these words, we used the most common part of speech to categorize concepts. This choice was predominately for simplicity of categorization, however, the participants were shown concepts without the suggestion of which sense to use for the word. Therefore, multiple senses are embedded into the feature production norms, while the database is labeled with single parts of speech. The other parts of speech can be found in the English Lexicon Project or multiple other databases. This dataset was combined with @McRae2004 and @Vinson2008 feature production norms, which was a combined total of `r nrow(partofspeech)` concepts. `r POS["noun"]`% of concepts were nouns, `r POS["adjective"]`% adjectives, `r POS["verb"]`% verbs, and `r POS["other"]`% were other forms of speech, such as adverbs and conjunctions.

## Procedure

Each HIT was kept to five concepts, and usual survey response times were five to seven minutes. Each HIT was open until thirty participants had successfully completed the HIT and were paid the five cents for the HIT. The survey instructions were copied from @McRae2005's Appendix B, which were also used in the previous publication of these norms. Because the @McRae2005 data was collected on paper, we modified these instructions slightly. The original lines to write in responses were changed to an online textbox response window. The detailed instructions additionally no longer contained information about how a participant should only consider the noun of the target concept, as the words in our study included multiple forms of speech and senses. Participants were encouraged to list the properties or features of each concept in the following areas: physical (looks, sounds, and feels), functional (uses), and categorical (belongings). The same examples used previously (*duck, cucumber, stove*) were included to aid in task understanding and completion. Participants signed up for the HITS through Amazon's Mechanical Turk website and completed the study within the Mechanical Turk framework. Approved HITs were compensated through the Mechanical Turk system. All answers were then combined into a larger dataset. 

## Data Processing

The entire dataset, at each processing stage described here, can be found at: https://osf.io/cjyzw/. On our OSF page, we have included a detailed processing guide on how concepts were (re)examined for this publication. This paper was written with *R* markdown and *papaja* [@R-papaja]. The markdown document allows an interested reader to view the scripts that created the article in line with the written text. However, the processing of the text documents was performed on the raw files, and therefore, we have included the processing guide for transparency of each stage. 

First, each concept was separated into an individual text file that is included as the "raw" data online. Each of these files was then spell checked and corrected when the participant answer was obviously a typo. As noted earlier, participants often tried to cut and paste Wikipedia or other online dictionary sources into the their answers to complete surveys quickly with minimal effort. These entries were easily found by the formatting of the webpage that was included in their answer. These answers were then discarded from the concept individual text files. Next, each concept was processed for feature frequency. In this stage, the raw frequency counts of each cue-feature combination were calculated and put together into one large file. Cue-cue combinations were discarded, as participants might write "a zebra is a horse" when asked to define *zebra*. English stop words such as *the, an, of* were then discarded, as well as terms that were often used as part of a definition (*like, means, describes*). 

To create the final root cosine values, we then created a "translated" column for each feature listed. This column indicated the root word for each feature, along with the affixes that were used in the original feature. For example, the original feature *cats* would be translated to *cat* and *s*, wherein *cat* would be the translated feature and the *s* would be the affix code. Multiple affix codes were often needed for features, as *beautifully* would have been translated to *beauty*, *ful*, and *ly*. Often, the noun version of the feature would be used for the translation or the most common part of speech for each feature would be recorded. The sample size for the cue was added to this dataset, as the sample sizes varied across experiment time, as shown in Table \@ref(tab:part-table). Therefore, instead of using raw feature frequency, we normalized each count into a percent of participants that included that feature with each cue. 

At this stage, the data was reduced to cue-feature combinations that were listed by at least 16% of participants (matching @McRae2005's procedure) or were in the top five features listed for that cue. This calculation was performed on the translated normalized feature percent. For example, *beauty* may have been listed as *beauty, beautiful, beautifully, beautifulness*, and this feature would have been listed three times in the dataset for the original cue. The *frequency_feature* column indicates the frequency of the original, unedited feature, while the *frequency_translated* includes all combinations of *beauty* into one overall feature. Because non-nouns can be more difficult to create a feature list for, we included the top five descriptors in addition to the 16% listed criteria, to ensure that each concept included at least five features. Table \@ref(tab:feature-table) indicates the average number of cue-feature pairs found for each data collection site/time point and part of speech for the cue word. 

The parts of speech for the cue, original feature, and translated feature were merged with this file as described above. Table \@ref(tab:percent-table) depicts the pattern of feature responses for cue-feature part of speech combinations. This table includes the percent of features listed for each cue-feature part of speech combination (i.e., what is the percent of time that both the cue and feature were both adjectives) for the original feature (raw) and translated feature (root). Next, the normalized frequency percent average was calculated along with their standard deviations. These columns indicate the frequency percent that a cue-feature part of speech combination was listed across participants (i.e., what is the average percent of participants that listed an adjective feature for an adjective cue). These two types of calculation describe the likelihood of seeing part of speech combinations across the concepts, along with the likelihood of those cue-feature part of speech combinations across participants. 

```{r feature-table, echo=FALSE, results='asis'}
##table
stim.table = matrix(NA, nrow = 6, ncol = 6)

colnames(stim.table) = c("Institution", "Adjective",
                         "Noun", "Verb", "Other", "Total")

stim.table[1, ] = c("University of Mississippi", "5.57 (1.53)", "7.35 (4.05)",  "5.33 (.87)", "6.01 (2.11)", "6.71 (3.44)")
stim.table[2, ] = c("Missouri State University", "5.74 (1.56)", "6.85 (2.82)", "6.67 (2.08)", "7.45 (5.35)", "6.65 (2.92)")
stim.table[3, ] = c("Montana State University", "5.81 (1.74)", "7.25 (3.35)", "5.59 (1.13)", "5.76 (1.74)", "6.69 (2.93)")
stim.table[4, ] = c("Mechanical Turk 1", "6.27 (2.28)", "7.74 (4.34)", "5.77 (1.17)", "5.57 (1.40)", "7.14 (3.79)")
stim.table[5, ] = c("Mechanical Turk 2", "5.76 (1.36)", "6.62 (1.85)", "5.92 (1.38)", "5.78 (1.17)", "6.38 (1.75)")         
stim.table[6, ] = c("Total", "5.78 (1.61)", "6.94 (2.88)", "5.67 (1.18)", "5.84 (1.71)", "6.57 (2.60)")

apa_table.latex(as.data.frame(stim.table),
                align = c(rep("l", 1), rep("c", 5)), 
                caption = "",
                note = "",
                escape = FALSE,
                col.names =  c("Institution", "Adjective",
                         "Noun", "Verb", "Other", "Total"))

```

```{r calculate-percent, include=FALSE}

####Set up for response frequency table####
##this code came from what we did last time

library(memisc) ##percent function comes from memisc library

##total
master = sub1

p0a = percent(master$pos_feature) #raw
p0a

p0b = percent(master$pos_translated) ##root
p0b

m0a = tapply(master$normalized_feature,
             master$pos_feature, mean)
m0a

sd0a = tapply(master$normalized_feature,
              master$pos_feature, sd)
sd0a

m0b = tapply(master$normalized_translated,
             master$pos_translated, mean)
m0b

sd0b = tapply(master$normalized_translated,
              master$pos_translated, sd)
sd0b

##subsetting by cue type
adj = subset(master,
             master$pos_cue == "adjective")

noun = subset(master,
              master$pos_cue == "noun")

verb = subset(master,
              master$pos_cue == "verb")

other = subset(master,
               master$pos_cue == "other")              

##adjectives
p1a = percent(adj$pos_feature) ##raw
p1a

p1b = percent(adj$pos_translated) ##root
p1b

m1a = tapply(adj$normalized_feature,
             adj$pos_feature, mean)
m1a

sd1a = tapply(adj$normalized_feature,
              adj$pos_feature, sd)
sd1a

m1b = tapply(adj$normalized_translated,
             adj$pos_translated, mean)
m1b

sd1b = tapply(adj$normalized_translated,
              adj$pos_translated, sd)
sd1b

##nouns
p2a = percent(noun$pos_feature)
p2a

p2b = percent(noun$pos_translated)
p2b

m2a = tapply(noun$normalized_feature,
            noun$pos_feature, mean)
m2a

sd2a = tapply(noun$normalized_feature,
              noun$pos_feature, sd)
sd2a

m2b = tapply(noun$normalized_translated,
             noun$pos_translated, mean)
m2b

sd2b = tapply(noun$normalized_translated,
              noun$pos_translated, sd)
sd2b

##verbs
p3a = percent(verb$pos_feature)
p3a

p3b = percent(verb$pos_translated)
p3b

m3a = tapply(verb$normalized_feature,
             verb$pos_feature, mean)
m3a

sd3a = tapply(verb$normalized_feature,
              verb$pos_feature, sd)
sd3a

m3b = tapply(verb$normalized_translated,
             verb$pos_translated, mean)
m3b

sd3b = tapply(verb$normalized_translated,
              verb$pos_feature, sd)
sd3b

##other
p4a = percent(other$pos_feature)
p4a

p4b = percent(other$pos_translated)
p4b

m4a = tapply(other$normalized_feature,
             other$pos_feature, mean)
m4a

sd4a = tapply(other$normalized_feature,
              other$pos_feature, sd)
sd4a

m4b = tapply(other$normalized_translated,
             other$pos_translated, mean)
m4b

sd4b = tapply(other$normalized_translated,
              other$pos_translated, sd)
sd4b

```

```{r percent-table, echo=FALSE, results='asis'}

####response frequency table####
response.table = matrix(NA, nrow = 20, ncol = 6)

#colnames(response.table) = c("Cue Type", "Feature Type",
                         #"%Raw", "%Root", "$M$ Freq. Raw", "$M$ Freq. Root")

response.table[1, ] = c("Adjective", "Adjective", 38.09, 29.74, "17.84 (16.47)", "30.02 (18.83)")
response.table[2, ] = c(" ", "Noun", 40.02, 46.74, "13.14 (14.96)", "29.71 (19.94)")
response.table[3, ] = c(" ", "Verb", 17.69, 20.72, "8.51 (9.78)", "26.88 (17.27)")
response.table[4, ] = c(" ", "Other", 4.20, 2.80, "15.17 (15.64)", "28.04 (15.54)")
response.table[5, ] = c("Noun", "Adjective", 16.56, 12.07, "15.55 (15.17)", "31.20 (18.17)")
response.table[6, ] = c(" ", "Noun", 60.85, 62.67, "17.21 (17.01)", "33.26 (20.05)")
response.table[7, ] = c(" ", "Verb", 20.80, 23.68, "8.88 (9.73)", "31.01 (17.87)" )
response.table[8, ] = c(" ", "Other", 1.79, 1.58, "17.06 (15.29)", "28.87 (17.14)")
response.table[9, ] = c("Verb", "Adjective", 15.16, 12.27, "13.95 (13.98)", "30.03 (18.28)")
response.table[10, ] = c(" ", "Noun", 42.92, 44.35, "14.59 (14.92)", "29.59 (18.90)")
response.table[11, ] = c(" ", "Verb", 36.92, 39.72, "12.75 (14.85)", "30.43 (19.54)")
response.table[12, ] = c(" ", "Other", 5.00, 3.66, "19.16 (15.95)", "25.59 (19.54)")
response.table[13, ] = c("Other", "Adjective", 20.80, 20.32, "16.61 (17.37)", "31.66 (19.51)")
response.table[14, ] = c(" ", "Noun", 42.74, 39.03, "16.77 (19.41)", "37.28 (25.94)")
response.table[15, ] = c(" ", "Verb", 19.66, 23.93,"7.18 (7.57)", "26.14 (19.38)")
response.table[16, ] = c(" ", "Other", 16.81, 16.71, "22.72 (16.69)", "30.70 (18.48)")
response.table[17, ] = c("Total", "Adjective", 19.74, 14.93, "16.12 (15.57)", "30.75 (18.37)")
response.table[18, ] = c(" ", "Noun", 55.41, 57.81, "16.55 (16.74)", "32.58 (20.09)")
response.table[19, ] = c(" ", "Verb", 22.02, 24.95, "9.50 (10.91)", "30.29 (18.24)")
response.table[20, ] = c(" ", "Other", 2.82, 2.31, "17.76 (15.83)", "28.45 (16.83)")

apa_table.latex(as.data.frame(response.table),
                align = c(rep("l", 2), rep("c", 4)), 
                caption = "",
                note = "",
                escape = FALSE,
               col.names =  c("Cue Type", "Feature Type",
                         "\\% Raw", "\\% Root", "$M$ Freq. Raw", "$M$ Freq. Root"))
```

The top cue-feature combinations from @Buchanan2013 and this new data collection were then combined with the cue-feature combinations from @McRae2005 and @Vinson2008. We did not reduce their cue-feature combinations, but instead included them with the cue-feature listed in their supplemental files with the feature in the raw feature column. If features could be translated into root words with affixes, the same procedure as described above was applied. The final file then included the original dataset, cue, feature, translated feature, frequency of the original feature, frequency of the translated feature, sample size, normalized frequencies for the original and translated feature. This file includes `r nrow(dat1)` cue-feature combinations, with `r nrow(sub1)` from our dataset, and `r nrow(sub1.reduce)` of which are unique cue-translated feature combinations. Statisics in Tables \@ref(tab:feature-table) and \@ref(tab:percent-table) only include information from the reprocessed @Buchanan2013 norms and the new cues collected for this project. The final data processing step was to code affixes found on the original features. A complete affix list translation file can be found online in our OSF files. Table \@ref(tab:affix-table) displays the list of affix tags, common examples for each type of affix, and the percent of affixes that fell into each cateory. The percent values are calculated on the overall affix list, as feature words could have up to three different affixes. Generally, affixes were tagged in a one-to-one match, however, special care was taken with numbers and verb tenses. Features like cat*s* would be coded as a number affix, while features like walk*s* would be coded as a third person verb. In the final words file found online, we additionally added forward strength (FSG) and backward strength (BSG) for investigation into association overlap. The last few columns indicate the word list a concept was originally normed in to allow for matching to the original raw files on the OSF page, along with the code for each school and time point of collection.  

```{r affix-calculation, include=FALSE}
####tag examples####
##code below is from what we did last time

library(reshape)

##Creating Affix dataset
affixdata = dat1[,c(2,13:15)]

##Melting dataset
longdata = melt(affixdata,
                id = "cue",
                measured = c("a1", "a2", "a3"))

colnames(longdata) = c("cue", "order", "affix")

summary(longdata)

##Removing zero affixes##
realaffix = subset(longdata, affix != 0)
summary(realaffix)

##Actual percentages for real
final = realaffix
options(scipen = 999)
affixtable = percent(droplevels(final$affix))
affixtable

```

```{r affix-table, echo=FALSE, results='asis'}
####making the affix table####

affix.table = matrix(NA, nrow = 13, ncol = 3)

colnames(affix.table) = c("Affix Tag", "Example",
                         "Percent")

affix.table[1, ] = c("Actions/Processes", "ion, ment, ble, ate, ize", 8.21)
affix.table[2, ] = c("Characteristic", "y, ous, nt, ful, ive, wise", 22.72)
affix.table[3, ] = c("Location", "under, sub, mid, inter", 0.44)
affix.table[4, ] = c("Magnitude", "er, est, over, super, extra", 1.31)
affix.table[5, ] = c("Not", "less, dis, un, non, in , im, ab", 2.76)
affix.table[6, ] = c("Number", "s, uni, bi, tri, semi", 28.31)
affix.table[7, ] = c("Opposites/Wrong", "mis, anti, de", 0.13)
affix.table[8, ] = c("Past Tense", "ed", 8.03)
affix.table[9, ] = c("Person/Object", "er, or, men, person, ess, ist", 7.23)
affix.table[10, ] = c("Present Participle", "ing", 14.03)
affix.table[11, ] = c("Slang", "bros, bike, bbq, diff, h2o", 0.12)
affix.table[12, ] = c("Third Person", "s", 6.16)
affix.table[13, ] = c("Time", "fore, pre, post, re", 0.54)

apa_table.latex(as.data.frame(affix.table),
                align = c(rep("l", 2), rep("c", 1)), 
                caption = "",
                note = "",
                escape = FALSE,
               col.names =  c("Affix Tag", "Example",
                         "Percent"))
```

This procedure is a slight departure from our previous work, as we previously argued to keep some morphologically similar features separate if they denoted different concepts. For example, *act* and *actor* were separated because each feature explained a separate component of the cue word (i.e., noun and gender). The original processing in @Buchanan2013 combined features that overlapped in cue sets by 80%. In this reprocessing and update, we translated all words to a root form, and coded these translations, thus, allowing for the exploration of the affect of affixes on semantic feature overlap. Both forms of the feature are provided for flexibility in calculating overlap by using the original feature (raw), the translated feature (root), and the affix overlap by code (affix). Cosine values were calculated for each of these feature sets by using the following formula:

$$
\frac{\sum_{i=1}^{n} A_i \times B_i} {\sqrt{\sum_{i=1}^{n} A_i^2} \times \sqrt{\sum_{i=1}^{n} B_i^2}}
$$

This formula is similar to a dot-product correlation, where $A_i$ and $B_i$ indicate the overlapping feature normalized frequency between cue A and cue B. The *i* subscript denotes the current cue, and when features match, the frequencies are multiplied together and summed across all matches ($\Sigma$). For the denominator, the feature normalized frequency is first squared and summed from *i* to *n* features for cue A and B. The square root of these summation values is then multiplied together. In essence, the numerator calculates the overlap of feature frequency for matching features, while the denominator accounts for the entire feature frequency set for each cue. Cosine values range from 0 (no overlapping features) to 1 (complete overlapping features). With nearly five thousand cue words, just under twenty-five million cue-cue cosine combinations can be calculated. In the datasets presented online, we only included cue-cue combinations with a feature overlap of at least two features, in order to reduce the large quantity of zero and very low cosine values. This procedure additionally allowed for online presentation of the data, as millions of lines was not feasible for our server. The complete feature list, along with our code to calculate cosine, can be used to obtain values not presented in our data if necessary.

## Website

In addition to our OSF page, we present a revamped website for this data at http://www.wordnorms.com/. The single words page includes information about each of the cue words including cue set size, concreteness, word frequency from multiple sources, length, full part of speech, orthographic/phonographic neighborhood, and number of phonemes, syllables, and morphemes. These values were taken from @Nelson2004, @Balota2007, and @Brysbaertsubtitle. A definition of each of these variables is provided along with the minimum, maximum, mean, and standard deviation of numeric values. The table is programmed using Shiny apps [@R-shiny]. Shiny is an *R* package that allows the creation of dynamic graphical user interfaces for interactive web applications. The advantage to using Shiny applications is data manipulation and visualization with the additional bonus of up to date statistics for provided data (i.e., as typos are fixed or data is updated, the web app will display the most recent caculations). In addition to the variable table, users can search and save filtered output using our Shiny search app. With this app, you can filter for specific variable ranges and save the output in a csv or Excel file. The complete data is also provided for download.

On the word pairs page, all information about word-pair statistics can be found. A second variable table is provided with semantic and associative statistics. This dataset includes the cue and target words from this project (cue-cue combinations), the root, raw, and affix cosines described above, as well as the original @Buchanan2013 cosines. Additional semantic information includes Latent Semantic Analysis [LSA; @Landeuar] and JCN [@Jiang] values provided in the @Maki2004 norms, along with FSG and BSG from the @Nelson2004 norms for association. The descriptions, minimum, maximum, mean, and standard deviations of these values are provided in the app. Again, the search app includes all of these stimuli for cue-cue combinations with two or more features in common, where you can filter this data for experimental stimuli creation. The separation of single and word-pair data (as well as cosine calculation reduction to cues with two features in common) was practical, as the applications run slowly as a factor of the number of rows and columns. On each page, we link the data, applications, and source code so that others may use and manipulate our work depending on their data creation or visualization goals.

# Results

An examination of the results of the cue-feature lists indicated that the new data collection is similar to the previous semantic feature production norms. As shown in Table \@ref(tab:feature-table), the new Mechanical Turk data showed roughly the same number of listed features for each cue concepts, usually between five to seven features. Table \@ref(tab:percent-table) portrayed that adjective cues generally included other adjectives or nouns as features, while noun cues were predominately described by other nouns. Verb cues included a large feature list of nouns, but then was equally split between adjectives, other verbs, and other categories. Lastly, the other cue types generally elicited nouns and verbs. Normalized percent frequencies were generally between seven and twenty percent of the participant sampling listing features when examining the raw words. These words included multiple forms, as the percent increased to around thirty percent when features were translated into their root words. Indeed, nearly half of the `r nrow(sub1)` cue-feature pairs were repeated, as `r nrow(sub1.reduce)` cue-feature pairs were unique when examining translated features. 

`r nrow(realaffix)` affix values were found, which was for `r length(unique(realaffix$cue))` cue concepts. `r table(realaffix$order)["a1"]` first affixes were found, with `r table(realaffix$order)["a2"]` second place affixes, and `r table(realaffix$order)["a3"]` third place affixes. Table \@ref(tab:affix-table) shows the distribution of these affix values. Generally, numbers were the largest category of affixes indicating that participants often indicated the quantity of the feature when describing the cue word. The second largest affix category was characteristics which often indicated the switch to or from a noun form of the feature word. Verb tenses (past tense, present participle, and third person) comprised a large set of affixes indicating the type of concept or when a concept might be doing an action for a cue. Persons and objects were also indicated about 7% of the time, while actions and processes of the cue were mentioned about 8% of the time. 

## Divergent Validity

```{r divergent-table, echo=FALSE, results='asis'}

####fsg/bsg table will go here####
##can use sub1 dataset from other tables
##Percent goes below

fsg.table = as.data.frame(matrix(NA, nrow = 5, ncol = 6))
colnames(fsg.table) = c("stim", "overlap", "mean", "sd", "min", "max")

percentNOTmiss = function(x){ sum(is.na(x) == FALSE)/length(x) *100 }

#cue translated FSG but remove the duplicates due to the translation
fsgtest = unique(dat1[ , c("FSG", "pos_cue", "cue", "translated")])
fsgtest$pos_cue = factor(fsgtest$pos_cue,
                         levels = c("adjective", "noun", "verb", "other"),
                         labels = c("Adjective", "Noun", "Verb", "Other"))

overlap = tapply(fsgtest$FSG, fsgtest$pos_cue, percentNOTmiss)
meanfsg = tapply(fsgtest$FSG, fsgtest$pos_cue, mean, na.rm = T)
sdfsg = tapply(fsgtest$FSG, fsgtest$pos_cue, sd, na.rm = T)
minfsg = tapply(fsgtest$FSG, fsgtest$pos_cue, min, na.rm = T)
maxfsg = tapply(fsgtest$FSG, fsgtest$pos_cue, max, na.rm = T)

overlap;meanfsg;sdfsg;minfsg;maxfsg

fsg.table$stim = c(levels(fsgtest$pos_cue), "Total")
fsg.table$overlap = apa(c(overlap, percentNOTmiss(fsgtest$FSG)),2)
fsg.table$mean = apa(c(meanfsg, mean(fsgtest$FSG, na.rm = T)), 2, F)
fsg.table$sd = apa(c(sdfsg, sd(fsgtest$FSG, na.rm = T)), 2, F)
fsg.table$min = apa(c(minfsg, min(fsgtest$FSG, na.rm = T)), 2, F)
fsg.table$max = apa(c(maxfsg, max(fsgtest$FSG, na.rm = T)), 2, F)
  
apa_table.latex(fsg.table,
                align = c(rep("l", 1), rep("c", 5)), 
                caption = "Percent and Mean Overlap between cue-feature lists from the free association norms",
                escape = FALSE,
               col.names =  c(" ", "\\% Overlap", "$M$ FSG", "$SD$ FSG", "Min", "Max"))

```

When collecting semantic feature production norms, there can be a concern that the information produced will simply mimic the free association norms, and thus, be a representation of association (context) rather than semanticity (meaning). Table \@ref(tab:divergent-table) portrays the overlap with the @Nelson2004 norms. The percent of time a cue-feature combination was present in the free association norms was calculated, along with the average FSG for those overlapping pairs. These values were calculated on the complete dataset with the @McRae2005 and @Vinson2008 norms, as we are presenting them as a combined dataset, on the translated cue-feature set only. The overall overlap between the database cue-feature sets and the free association cue-target sets was approximately 37%, ranging from 32% for verbs and nearly 52% for adjectives. Similiar to our previous results, the range of the FSG was large (.01 - .94), however, the average FSG was low for these overlapping pairs, *M* = .11 (*SD* = .14). These results indicated that while it will always be difficult to separate association and meaning, the dataset presented here represents a low association when examining overlapping values, and more than 60% of the data is completely separate from the free association norms. The limitation to this finding is the removal of idiosyncratic responses from the @Nelson2004 norms, but even if these were to be included in some form, the average FSG would still be quite low when comparing cue-feature lists to cue-target lists. 

## Convergent Validity

```{r convergent, include = FALSE}
cosmatch = read.csv("cosine_matches.csv")
b = subset(cosmatch, where1 == "b") 
m = subset(cosmatch, where1 == "m")  

meanroot = tapply(cosmatch$V3, list(cosmatch$where1, cosmatch$where2), mean)
meanraw = tapply(cosmatch$V4, list(cosmatch$where1, cosmatch$where2), mean)
meanaffix = tapply(cosmatch$V5, list(cosmatch$where1, cosmatch$where2), mean)  

sdroot = tapply(cosmatch$V3, list(cosmatch$where1, cosmatch$where2), sd)
sdraw = tapply(cosmatch$V4, list(cosmatch$where1, cosmatch$where2), sd)
sdaffix = tapply(cosmatch$V5, list(cosmatch$where1, cosmatch$where2), sd)  
```

To examine the validity of cosine values, we calculated the average cosine score between the new processing of the data for each of the three feature production norms used in this project. Overlapping cues in each of three database sets were found (*n* = `r length(unique(cosmatch$cue))`), and the average cosine between their feature sets was examined. @Buchanan2013 and the new dataset are listed the subscript B, while @McRae2005 is M and V for @Vinson2008. For root cosine values, we found high overlap between all three datasets: $M_{BM}$ = `r apa(meanroot[1], 2, F)` (*SD* = `r apa(sdroot[1], 2, F)`), $M_{BV}$ = `r apa(meanroot[3], 2, F)` (*SD* = `r apa(sdroot[3], 2, F)`), and $M_{MV}$ = `r apa(meanroot[4], 2, F)` (*SD* = `r apa(sdroot[4], 2, F)`). The raw cosine values also overlapped well, even though the @McRae2005 and @Vinson2008 datasets were already mostly preprocessed for word stems: $M_{BM}$ = `r apa(meanraw[1], 2, F)` (*SD* = `r apa(sdraw[1], 2, F)`), $M_{BV}$ = `r apa(meanraw[3], 2, F)` (*SD* = `r apa(sdraw[3], 2, F)`), and $M_{MV}$ = `r apa(meanraw[4], 2, F)` (*SD* = `r apa(sdraw[4], 2, F)`). Last, the affix cosines overlapped well between BM datasets, $M_{BM}$ = `r apa(meanaffix[1], 2, F)` (*SD* = `r apa(sdaffix[1], 2, F)`), but did not overlpa with the V datasets: $M_{BV}$ = `r apa(meanaffix[3], 2, F)` (*SD* = `r apa(sdaffix[3], 2, F)`), and $M_{MV}$ = `r apa(meanaffix[4], 2, F)` (*SD* = `r apa(sdaffix[4], 2, F)`). Last, the correlation between root, raw, affix, old cosine, LSA, and JCN were calculated to examine convergent validity. As shown in Table \@ref(tab:correlation-table), the intercorrelations between the cosine measures are high, especially between our previous work and this dataset. JCN is backwards coded, as zero values indicate close semantic neighbors (low dictionary distance) and high values indicate low semantic relation. The small negative correlations replicate previous findings. LSA values showed small positive correlations with cosine values, indicating some overlap with thematic information and semantic feature overlap [@Maki2008]. These correlations are slightly different than our previous publication, likely because we restricted this cosine set to values with at least two features in common. The results are simiilar, where LSA and JCN correlations are lower than LSA-COS and JCN-COS, but these values indicate that themes and dictionary distance are similarly related to feature overlap. 

```{r correlations, eval=FALSE, eval=FALSE, include=FALSE}

####Correlation table goes here####

dat.cor = averaged_cosine ##imported via file viewer, acc.csv

dat.cor$jcn = as.numeric(dat.cor$jcn)
dat.cor$fsg = as.numeric(dat.cor$fsg)
dat.cor$oldcos = as.numeric(dat.cor$oldcos)
dat.cor$lsa = as.numeric(dat.cor$lsa)
dat.cor$bsg = as.numeric(dat.cor$bsg)

cor(dat.cor[ , -c(1:2)], use = "pairwise.complete.obs")

```

```{r correlation-table, echo=FALSE, results='asis'}

cor.table = matrix(NA, nrow = 8, ncol = 9)

colnames(cor.table) = c(" ", "Root", "Raw", "Affix", "Previous COS", "JCN", "LSA", "FSG", "BSG")

cor.table[1, ] = c("Root", 1, " " , " " , " " , " " , " ", " ", " ")
cor.table[2, ] = c("Raw", .93, 1, " ", " ", " ", " ", " ", " ")
cor.table[3, ] = c("Affix", .50, .53, 1, " ", " ", " ", " ", " ")
cor.table[4, ] = c("Previous COS", .94, .91, .49, 1, " ", " ", " ", " ")
cor.table[5, ] = c("JCN", -.18, -.22, -.17, -.22, 1, " ", " ", " ")
cor.table[6, ] = c("LSA", .18, .15, .10, .21, -.06, 1, " ", " ")
cor.table[7, ] = c("FSG", .06, .04, .08, .10, -.15, .24, 1, " ")
cor.table[8, ] = c("BSG", .14, .15, .17, .18, -.18, .26, .31, 1)

apa_table.latex(as.data.frame(cor.table),
                align = c(rep("l", 1), rep("c", 8)), 
                caption = "",
                note = "",
                escape = FALSE,
               col.names =  c(" ", "Root", "Raw", "Affix", "Previous COS", "JCN", "LSA", "FSG", "BSG"))
```

## Relation to Semantic Priming

```{r ldt-table, echo=FALSE, results='asis'}
##load data sets
faldt.dat = read.csv("FALDT.csv")
fan.dat = read.csv("FAN.csv")
oaldt.dat = read.csv("OALDT.csv")
oan.dat = read.csv("OAN.csv")

#figure out the correlations of the datasets
#want columns 18, 19, 21, 29, 32, 49, 50, 51, 52, 86, 87, 88, 90, 91, 92

faldt.cor = cor(faldt.dat[ , c(18, 19, 21, 29, 32, 49, 50, 51, 83, 86, 87, 88, 90, 91, 92)], use = "pairwise.complete.obs")

fan.cor = cor(fan.dat[ , c(18, 19, 21, 29, 32, 49, 50, 51, 83, 86, 87, 88, 90, 91, 92)], use = "pairwise.complete.obs")

oaldt.cor = cor(oaldt.dat[ , c(18, 19, 21, 29, 32, 49, 50, 51, 83, 86, 87, 88, 90, 91, 92)], use = "pairwise.complete.obs")

oan.cor = cor(oan.dat[ , c(18, 19, 21, 29, 32, 49, 50, 51, 83, 86, 87, 88, 90, 91, 92)], use = "pairwise.complete.obs")

cor.table = as.data.frame(matrix(NA, nrow = 13, ncol = 5))
colnames(cor.table) = c("Variable", "FA-LDT 200", "FA-LDT 1200", "OA-LDT 200", "OA-LDT 1200")

cor.table[ , 1] = c("Root COS", "Raw COS", "Affix COS", 
                    "Target CSS", "Target Raw FSS", "Target Root FSS", 
                    "Cue CSS", "Cue Raw FSS", "Cue Root FSS", 
                    "FSG", "BSG", "LSA", "JCN")
cor.table[ , 2] = apa(faldt.cor[c(6,7,8,10:15,1,2,3,9) , 4],2,F)
cor.table[ , 3] = apa(faldt.cor[c(6,7,8,10:15,1,2,3,9) , 5],2,F)
cor.table[ , 4] = apa(oaldt.cor[c(6,7,8,10:15,1,2,3,9) , 4],2,F)
cor.table[ , 5] = apa(oaldt.cor[c(6,7,8,10:15,1,2,3,9) , 5],2,F)

##reorder a bit
cor.table = cor.table[ c(1:3, 6:4, 9:7, 10:13), ]

apa_table(as.data.frame(cor.table),
          align = c(rep("l", 1), rep("c", 4)), 
          caption = "LDT Response Latencies Correlation with Semantic and Associative Variables",
          note = "Missing values excluded pairwise for JCN.")
```

```{r name-table, echo=FALSE, results='asis'}

cor.table = as.data.frame(matrix(NA, nrow = 13, ncol = 5))
colnames(cor.table) = c("Variable", "FA-Name 200", "FA-Name 1200", "OA-Name 200", "OA-Name 1200")

cor.table[ , 1] = c("Root COS", "Raw COS", "Affix COS", 
                    "Target SS", "Target Raw FSS", "Target Root FSS", 
                    "Cue CSS", "Cue Raw FSS", "Cue Root FSS", 
                    "FSG", "BSG", "LSA", "JCN")
cor.table[ , 2] = apa(fan.cor[c(6,7,8,10:15,1,2,3,9) , 4],2,F)
cor.table[ , 3] = apa(fan.cor[c(6,7,8,10:15,1,2,3,9) , 5],2,F)
cor.table[ , 4] = apa(oan.cor[c(6,7,8,10:15,1,2,3,9) , 4],2,F)
cor.table[ , 5] = apa(oan.cor[c(6,7,8,10:15,1,2,3,9) , 5],2,F)

##reorder a bit
cor.table = cor.table[ c(1:3, 6:4, 9:7, 10:13), ]

apa_table(as.data.frame(cor.table),
          align = c(rep("l", 1), rep("c", 4)), 
          caption = "Naming Response Latencies Correlation with Semantic and Associative Variables",
          note = "Missing values excluded pairwise for JCN.")
```

As a second examination of convergent validity, the correlation between values calculated from these norms and the *Z* priming values from the Semantic Priming Project were examined. The Semantic Priming Project includes lexical decision and naming response latencies for priming at 200 and 1200 ms stimulus onset asychronies (SOA). In these experiments, participants were shown cue-target words that were either the first associate of a concept or an other associate (second response or higher in the @Nelson2004 norms). The response latency of the target word was subtracted from the non-primed lexical decision or naming time using the English Lexicon Project as the baseline expected response latencies for concepts. Therefore, each target item received four (two SOAs by two tasks) priming times, and we selected the *Z*-scored priming from the dataset to correlate with our data. In addition to root, raw, and affix cosine, we additionally calculated feature set size for the cue and target of the primed pairs. Feature set size is the number of features listed by participants when creating the norms for that concept. Because of the nature of our norms, we calculated both feature set size for the raw, untranslated features, as well as the translated features. The average feature set sizes for our dataset can be found in Table \@ref(tab:feature-table). The last variable included was cosine set size which was defined as the number of other concepts each cue or target was (nonzero) paired with in the cosine values. Feature set size indicates the number of features listed for each cue or target, while cosine set size indicates the number of other semantically related concepts for each cue or target.   

Tables \@ref(tab:ldt-table) and \@ref(tab:name-table) display the correlations between the new semantic variables described above, as well as FSG, BSG, LSA, and JCN for reference. For lexical decision priming, we found small correlations between the root and raw cosine values and priming, with the largest for first associates in the 200 ms condition. The correlations decreased for the 1200 ms condition and the other associate SOAs. These two variables are highly correlated, therefore, it is not surprising that they have similiar correlations with priming. Affix cosine also was related priming in a small way, especially for first associates in the 200 ms condition. Most of the cue and feature set sizes were not related to priming, showing correlations close to zero in most instances. Cue set size for the cue word was somewhat related to 200 ms priming, along with raw cue feature set size. These correlations are small, but they are comparable or greater than the correlations for association and other measures of semantic or thematic relatedness. For naming, the results are less consistent. Cosine values are related to 1200 ms naming in first and other associates, and none of the feature or cue set sizes showed any relationship with priming. Again, we see that many of the other associative and semantic variables correspondingly do not correlate with priming. In both naming and lexical decision priming, BSG has a small but consistent relationship with priming, which may indicate the processing of the target back to the cue. LSA was also a small predictor of priming across conditions.   

# Discussion

This research project focused on expanding the avaliability of English semantic feature overlap norms, in an effort to provide more coverage of concepts that occur in other large database projects like the Semantic Priming and English Lexicon Projects. The number and breadth of linguistic variables and normed databases has increased over the years, however, researchers can still be limited by the concept overlap between them. Projects like the Small World of Words provide newly expanded datasets for association norms, and our work helps fill the voids for corresponding semantic norms. To provide the largest dataset of similiar data, we combined the newly collected data with previous work by using @Buchanan2013, @McRae2004, and @Vinson2008 together. These norms were reprocessed from previous work to explore the impact of coding system for feature overlap. As shown in the correlation between root and raw cosines, the parsing of words to root form creates very similiar results, and each results similiarly correlated with other variables. This result does not imply that these values are the same, as root cosines were larger than their corresponding raw cosine. It does, however, imply that the cue-feature coding can produce similiar results in raw or translated format.

Of particular interest was the information that is often lost when translating raw features back to a root word. One surprising result in this study was the sheer number of affixes present on each cue word. With these values, we believe we have captured some of the nuisance that is often discarded in this type of research. Affix cosines were less related to their feature root and raw counterparts, but also showed small correlations with semantic priming. Potentially, affix overlap can be used to add small, but important predictive value to related semantic phenomena.    

Further investigation into the compound prediction of these variables is warrented to fully explore how these, and other lexical variables, may be used to understand semantic priming. An examination of the cosine values from the Semantic Priming Project cue-target set indicates that these values were low, with many zeros. This restriction of range could explain the small correlations with priming, along with the understanding that semantic priming can be exceedingly variable and small across items. 


\newpage

# References
```{r create_r-references}
r_refs(file = "r-references.bib")
```

\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}
