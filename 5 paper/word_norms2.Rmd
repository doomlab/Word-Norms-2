---
title             : "English Semantic Feature Production Norms: An Extended Database of 4,436 Concepts"
shorttitle        : "Semantic Norms"

author: 
  - name          : "Erin M. Buchanan"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "901 S. National Ave, Springfield, MO 65897"
    email         : "erinbuchanan@missouristate.edu"
  - name          : "K. D. Valentine"
    affiliation   : "2"
  - name          : "Nicholas P. Maxwell"
    affiliation   : "1"

affiliation:
  - id            : "1"
    institution   : "Missouri State University"
  - id            : "2"
    institution   : "University of Missouri"

author_note: |
  Erin M. Buchanan is an Associate Professor of Quantitative Psychology at Missouri State University. K. D. Valentine is a Ph.D. candidate at the University of Missouri. Nicholas P. Maxwell is a Masters' candidate at Missouri State University.
  
  We would like to thank Keith Hutchison and David Balota for their contributions to this project, including the funds to secure Mechanical Turk participants. Additionally, we thank Simon de Deyne and an anonymous reviewer for their comments on this manuscript. 

abstract: |
  A limiting factor in understanding memory and language is often the availability of large numbers of stimuli to use and explore in experimental studies. In this study, we expand on three previous databases of concepts to over 4,000 words including nouns, verbs, adjectives, and other parts of speech. Participants in the study were asked to provide lists of features for each concept presented (a semantic feature production task), which were combined with the previous research in this area. These feature lists for each concept were then coded into their root word form and affixes (i.e., *cat* and *s* for *cats*) to explore the impact of word form on semantic similarity measures, which are often calculated by comparing concept feature lists (feature overlap). All concept features, coding, and calculated similarity information is provided in a searchable database for easy access and utilization for future researchers when designing experiments that use word stimuli. The final database of word pairs was combined with the Semantic Priming Project to examine the relation of semantic similarity statistics on semantic priming in tandem with other psycholinguistic variables. 
  
keywords          : "semantics, word norms, database, psycholinguistics"

bibliography      : ["r-references.bib", "wn2.bib"]

figsintext        : no
figurelist        : no
tablelist         : no
footnotelist      : no
lineno            : yes

lang              : "english"
class             : "man"
output            : papaja::apa6_pdf
replace_ampersands: yes
csl               : apa6.csl
---

```{r load_packages, include = FALSE}
library("papaja")
library(MOTE)
library(shiny) #to cite shiny
```

Semantic representations are the focus of a large area of research which tries to delineate the essential features of a concept. These features are key to models of semantic memory [i.e., memory for facts; @Collins1969; @Collins1975], and they have been used to create both feature based [@Smith1974; @Cree2003; @Vigliocco2004] and distributional based models [@Jones2007; @Griffiths2007; @Riordan2011]. Feature based models indicate that the degree of similiarity between concepts is defined by their overlapping feature lists, while distributional based models posit that similiarity is defined by the overlap between linguistic network or context. To create feature based similiarity, participants were often asked to create lists of properties for categories of words. This property listing was a seminal task with corresponding norms that have been prevalent in the literature [@Toglia1978; @Toglia2009; @Rosch1975; @Ashcraft1978a]. Feature production norms are created by soliciting participants to list properties or features of a target concept without focusing on category. These features are then compiled into feature sets that are thought to represent, at least somewhat, the memory representation of a particular concept. Previous work on semantic feature production norms in English includes databases by @Buchanan2013, @McRae2005, and @Vinson2008. 

For example, when queried on what features define a *cat*, participants may list *tail*, *animal*, and *pet*. These features capture the most common types of descriptions: "is a" and "has a". Additionally, feature descriptions may include uses, locations, behavior, and gender (i.e., *actor* denotes both a person and gender). The goal of these norms is often to create a set of high-probability features, as there can and will be many idiosyncratic features listed in this task, to explore the nature of concept structure. In the classic view of category structure, concepts have defining features or properties, while the probabilistic view suggests that categories are fuzzy with concepts that are typical of a concept [@Medin1989]. These norms have now been published in Italian [@Montefinese2013; @Reverberi2004], German [and Italian, @Kremer2011a], Portuguese [@Stein2009], Spanish [@Vivas2017], and Dutch [@Ruts2004], as well as for the blind [@Lenci2013].

The concepts presented in the feature production norming task are usually called *cues*, and this terminology also applies to the first related word presented in a semantic priming task. *Features* are concepts listed in response to a cue in the feature production task. In a semantic priming task, the concept paired with a cue is denoted as a *target*. In a lexical decision task, participants are shown cue words before a related or unrelated target word. Their task is to decide if the target word is a word or nonword as quickly as possible. A similar task, naming, involves reading the second target word aloud after viewing a related or unrelated cue word. Semantic priming occurs when the target word is recognized (responded to or read aloud) faster after the related cue word in comparison to the unrelated cue word [@Moss1995]. The feature list data created from the production task can be used to determine the strength of the relation between cue and target word, often by calculating the feature overlap, or number of shared features between concepts [@McRae2005]. Both the cue-feature lists and the cue-cue combinations (i.e., the relation between two cues in a feature production dataset, which becomes a cue-target combination in the priming task) are useful and important data for researchers in exploring various semantic based phenomena.

First, the feature category lists can provide insight into the probabilistic nature of language and conceptual structure [@Moss2002; @Cree2003; @McRae1997; @Pexman2003]. Next, the feature production norms can be used as the underlying data to create models of semantic priming and cognition focusing on cue-target relation [@Cree1999; @Rogers2004; @Vigliocco2004]. When using database norms to select for stimuli, others have studied semantic word-picture interference [i.e., slower naming times when distractor words are related category concepts in a picture naming task; @Vieth2014], recognition memory [@Montefinese2015], and semantic richness, which is a measure of shared defining features [@Grondin2009; @Kounios2009; @Yap2015; @Yap2016]. The Vinson and Vigliocco labs have shown the power of turning in-house data projects into a larger norming set [@Vinson2008], as they published papers on aphasia [i.e., the loss of understanding speech; @Vinson2002; @Vinson2003], meaning-syntactic differences [i.e., differences in naming times based on semantic or syntatic similiarity; @Vigliocco2002; @Vigliocco2005], and representational models [@Vigliocco2004].

However, it would be unwise to consider these norms as an exact representation of a concept in memory [@McRae2005]. These norms represent salient features that participants can recall, likely because saliency is considered special to our understanding of concepts [@Cree2003]. Additionally, @Barsalou2003 suggested that participants are likely creating a mental model of the concept based on experience and using that model to create a feature property list. This model may represent a specific instance of a category (i.e., their pet dog), and feature lists will represent that particular memory.

Computational modeling of memory requires sufficiently large datasets to accurately portray semantic memory, therefore, the advantage of big data in psycholinguistics cannot be understated. There are many large corpora that could be used for exploring the structure of language and memory through frequency [see the SUBTLEX projects @Brysbaert2009; @New2007]. Additionally, there are large lexicon projects that explore how the basic features of words affect semantic priming, such as orthographic neighborhood (words that are one letter different from the concept), length, and part of speech [@Balota2007; @Keuleers2012]. In contrast to these basic linguistic features of words, other norming efforts have involved subjective ratings of concepts. Large databases of age of acquisition [i.e., rated age of learning the concept; @Kuperman2012], concreteness [i.e., rating of how perceptible a concept is; @Brysbaert2013], and valence [i.e., rating of emotion in a concept; @Warriner2013] provide further avenues for understanding the impact these rated properties have on semantic memory. For example, age of acquisition and concreteness ratings have been shown to predict performance on recall tasks [@Dewhurst1998; @Brysbaert2013], while valence ratings are useful for gauging the effects of emotion on meaning [@Warriner2013]. These projects represent a small subset of the larger normed stimuli available [@Buchanan2018], however, research is still limited by the overlap between these datasets. If a researcher wishes to control for lexical characteristics and subjective rating variables, the inclusion of each new variable to the study will further restrict the item pool for study. Large, overlapping datasets are crucial for exploring the entire range of an effect, and to ensure that the stimuli set is not the only contributing factor to the results of a study. 

Therefore, the purpose of this study is to further expand the stimuli and variable options available to the field, as well as promote the use of these norms for stimuli creation. To accomplish these goals, we have expanded our original semantic feature production norms [@Buchanan2013] to include all cues and targets from The Semantic Priming Project [@Hutchison2013]. The existing norms were reprocessed along with these new norms to provide new feature coding and affixes (i.e., word addition that modifies meaning, such as *pre* or *ing*). The entire dataset is available on our website (http://wordnorms.com/) which has been revamped with a new interface and web applications to easily find and select stimuli for future experiments. The data collection, (re)processing, website, and finalized dataset are detailed below. 

# Method

## Participants

Participants in the newly collected stimuli set were gathered from Amazon's Mechanical Turk, which is a large, diverse participant pool wherein users can complete surveys for small sums of money [@Buhrmester2011]. Answers can be screened for errors, and incorrect or incomplete surveys can be rejected or discarded without payment. Each participant was paid five cents for a survey, and they could complete multiple Human Intelligence Tasks or HITS. Each HIT included five concepts, and HITS would remain active until *n* = 30 valid survey answers were collected. HITS were usually rejected if they included copied definitions from Wikipedia, "I don't know", or writing a paragraph about the concept. These answers were discarded, as described below. Table \@ref(tab:part-table) includes the sample sizes from the new study (Mechanical Turk 2), as well as the sample sizes from the previous study, as described in @Buchanan2013. 

```{r part-table, echo=FALSE, results='asis'}
####partcipant table####
participant.table2 = matrix(NA, nrow = 5, ncol = 4)

colnames(participant.table2) = c("Institution", "Total Participants",
                         "Concepts", "Mean $N$")

participant.table2[1, ] = c("University of Mississippi", 749, 658, 67.8)
participant.table2[2, ] = c("Missouri State University", 1420, 720, 71.4)
participant.table2[3, ] = c("Montana State University", 127, 120, 63.5)
participant.table2[4, ] = c("Mechanical Turk 1", 571, 310, 60)
participant.table2[5, ] = c("Mechanical Turk 2", 198, 1914, 30)         

apa_table(as.data.frame(participant.table2),
                align = c(rep("l", 1), rep("c", 3)), 
                caption = "Sample Size and Concept Norming Size for Each Data Collection Location/Time Point",
                col.names = c("Institution", "Total Participants",
                         "Concepts", "Mean $N$"))
```

## Materials

The purpose of this second norming set was to expand the @Buchanan2013 norms to include all concepts from the Semantic Priming Project [@Hutchison2013]. The original concept set was selected primarily from the @Nelson2004 database, with small overlaps in the @McRae2005 and @Vinson2008 database sets for convergent validity. In the Semantic Priming Project, cue-target pairs were shown to participants to examine naming (i.e., reading a concept aloud) and lexical decision (i.e., responding if a presented string is a word or nonword) response latency priming across related and unrelated pairs. The related pairs included first associate (most common response to a cue, *sum*-*add*) and other associates (second or greater common responses to cues, *safe*-*protect*) as their target words. The @Buchanan2013 publication of concepts included many of the cue words from the Semantic Priming Project, while this project expanded to include unnormed cue words and all target words for all first and other associate pairs. The addition of these concepts allowed for complete overlap between the Semantic Priming Project and feature production norms. 

```{r calculate-frequency, include=FALSE}

dat1 = read.csv("final words 2017.csv")
sub1 = subset(dat1,
              dat1$where == "b")

####stimuli table####

#we want the number of features for each concept
#reduce down to just the translated data
sub1.reduce = sub1[ , c("cue", "translated", "pos_cue", "school_code")]
sub1.reduce = unique(sub1.reduce)
sub1.reduce$cue = droplevels(sub1.reduce$cue)

#for each cue calculate the number of features listed
nofeatures = as.data.frame(table(sub1.reduce$cue))
colnames(nofeatures)[1] = "cue"

#add in the school code and pos_cue
finalfreq = unique(merge(nofeatures, sub1.reduce[ , c("cue", "pos_cue", "school_code")], by = "cue"))

#now calculate statistics - means by school
with(finalfreq, tapply(Freq, list(pos_cue, school_code), mean))
with(finalfreq, tapply(Freq, list(pos_cue, school_code), sd))

##mean frequency per school
with(finalfreq, tapply(Freq, list(school_code), mean))
with(finalfreq, tapply(Freq, list(school_code), sd))

##mean frequency by part of speech
with(finalfreq, tapply(Freq, list(pos_cue), mean))
with(finalfreq, tapply(Freq, list(pos_cue), sd))

##total total 
mean(finalfreq$Freq); sd(finalfreq$Freq)

partofspeech = unique(dat1[ , c("cue", "pos_cue")])
POS = apa(table(partofspeech$pos_cue)/nrow(partofspeech)*100,1)
```

Concepts were labeled by part of speech using the English Lexicon Project [@Balota2007], the free association norms, and Google's define search when necessary. When labeling these words, we used the most common part of speech to categorize concepts. This choice was predominately for simplicity of categorization, however, the participants were shown concepts without the suggestion of which sense to use for the word. Therefore, multiple senses (i.e., *bat* is noun and a verb) are embedded into the feature production norms, while the database is labeled with single parts of speech. The other parts of speech can be found in the English Lexicon Project or multiple other databases. This dataset was combined with @McRae2005 and @Vinson2008 feature production norms, which resulted in a combined total of `r nrow(partofspeech)` concepts. `r POS["noun"]`% of concepts were nouns, `r POS["adjective"]`% adjectives, `r POS["verb"]`% verbs, and `r POS["other"]`% were other forms of speech, such as adverbs and conjunctions.

## Procedure

Each HIT was kept to five concepts, and usual survey response times were between five to seven minutes. Each HIT was open until thirty participants had successfully completed the HIT and were paid the five cents for the HIT. The survey instructions were copied from @McRae2005's Appendix B, which were also used in the previous publication of these norms. Because the @McRae2005 data was collected on paper, we modified these instructions slightly. The original lines to write in responses were changed to an online text box response window. The detailed instructions additionally no longer contained information about how a participant should only consider the noun of the target concept, as the words in our study included multiple forms of speech and senses. Participants were encouraged to list the properties or features of each concept in the following areas: physical (looks, sounds, and feels), functional (uses), and categorical (belongings). The same examples used previously in @McRae2005 and @Buchanan2013 (*duck, cucumber, stove*) were included to aid in task understanding and completion. Participants signed up for the HITS through Amazon's Mechanical Turk website and completed the study within the Mechanical Turk framework. Approved HITs were compensated through the Mechanical Turk system. All answers were then combined into a larger dataset. 

## Data Processing

The entire dataset, at each processing stage described here, can be found at: https://osf.io/cjyzw/. On our OSF page, we have included a detailed processing guide on how concepts were (re)examined for this publication. This paper was written with *R* markdown [@R-base] and *papaja* [@R-papaja]. The markdown document allows an interested reader to view the scripts that created the article in line with the written text. However, the processing of the text documents was performed on the raw files, and therefore, we have included the processing guide for transparency of each stage. 

First, each concept was separated into an individual text file that is included as the "raw" data online. Each of these files was then spell checked and corrected when the participant answer was obviously a typo. As noted earlier, participants often tried to cut and paste Wikipedia or other online dictionary sources into the their answers to complete surveys quickly with minimal effort. These entries were easily found because the formatting of the webpage was included in their answer. These answers were then discarded from the individual concept's text file. Next, each concept was processed for feature frequency. In this stage, the raw frequency counts of each cue-feature combination were calculated and put together into one large file. Cue-cue combinations were discarded, as participants might write "a zebra is a horse" when asked to define *zebra*. English stop words such as *the, an, of* were then discarded, as well as terms that were often used as part of a definition (*like, means, describes*). 

We then created a "translated" column for each feature listed. This column indicated the root word for each feature, and additional columns were added with the affixes that were used in the original feature. For example, the original feature *cats* would be translated to *cat* and *s*, wherein *cat* would be the translated feature and the *s* would be the affix code. Multiple affix codes were often needed for features, as *beautifully* would have been translated to *beauty*, *ful*, and *ly*. Often, the noun version of the feature would be used for the translation or the most common part of speech for each feature would be recorded. The sample size for the cue was added to this dataset, as the sample sizes varied across experiment time, as shown in Table \@ref(tab:part-table). Therefore, instead of using raw feature frequency, we normalized each count into the percent of participants that included that feature with each cue. 

At this stage, the data was reduced to cue-feature combinations that were listed by at least 16% of participants (matching @McRae2005's procedure) or were in the top five features listed for that cue. This calculation was performed on the translated normalized feature percent. For example, *beauty* may have been listed as *beauty, beautiful, beautifully, beautifulness*, and this feature would have been listed four times in the dataset for the original cue. The *frequency_feature* column indicates the frequency of the original, unedited feature, while the *frequency_translated* includes all combinations of *beauty* into one overall feature. Because non-nouns can be more difficult to create a feature list for, we included the top five descriptors in addition to the 16% listed criteria, to ensure that each concept included at least five features. Table \@ref(tab:feature-table) indicates the average number of cue-feature pairs found for each data collection site/time point and part of speech for the cue word. 

The parts of speech for the cue, original feature, and translated feature were merged with this file as described above. Table \@ref(tab:percent-table) depicts the pattern of feature responses for cue-feature part of speech combinations. This table includes the percent of features listed for each cue-feature part of speech combination (i.e., what is the percent of time that both the cue and feature were both adjectives) for the original feature (raw) and translated feature (root). Next, the normalized frequency percent average was calculated along with their standard deviations. These columns indicate the frequency percent that a cue-feature part of speech combination was listed across participants (i.e., what is the average percent of participants that listed an adjective feature for an adjective cue). These two types of calculation describe the likelihood of seeing part of speech combinations across the concepts, along with the likelihood of those cue-feature part of speech combinations across participants. 

```{r feature-table, echo=FALSE, results='asis'}
##table
stim.table = matrix(NA, nrow = 6, ncol = 6)

colnames(stim.table) = c("Institution", "Adjective",
                         "Noun", "Verb", "Other", "Total")

stim.table[1, ] = c("University of Mississippi", "5.57 (1.53)", "7.35 (4.05)",  "5.33 (.87)", "6.01 (2.11)", "6.71 (3.44)")
stim.table[2, ] = c("Missouri State University", "5.74 (1.56)", "6.85 (2.82)", "6.67 (2.08)", "7.45 (5.35)", "6.65 (2.92)")
stim.table[3, ] = c("Montana State University", "5.81 (1.74)", "7.25 (3.35)", "5.59 (1.13)", "5.76 (1.74)", "6.69 (2.93)")
stim.table[4, ] = c("Mechanical Turk 1", "6.27 (2.28)", "7.74 (4.34)", "5.77 (1.17)", "5.57 (1.40)", "7.14 (3.79)")
stim.table[5, ] = c("Mechanical Turk 2", "5.76 (1.36)", "6.62 (1.85)", "5.92 (1.38)", "5.78 (1.17)", "6.38 (1.75)")         
stim.table[6, ] = c("Total", "5.78 (1.61)", "6.94 (2.88)", "5.67 (1.18)", "5.84 (1.71)", "6.57 (2.60)")

apa_table(as.data.frame(stim.table),
                align = c(rep("l", 1), rep("c", 5)), 
                caption = "Average (SD) Cue-Feature Pairs by Location/Time Point",
                col.names =  c("Institution", "Adjective",
                         "Noun", "Verb", "Other", "Total"))

```

```{r calculate-percent, include=FALSE}

####Set up for response frequency table####
##this code came from what we did last time

library(memisc) ##percent function comes from memisc library

##total
master = sub1

p0a = percent(master$pos_feature) #raw
p0a

p0b = percent(master$pos_translated) ##root
p0b

m0a = tapply(master$normalized_feature,
             master$pos_feature, mean)
m0a

sd0a = tapply(master$normalized_feature,
              master$pos_feature, sd)
sd0a

m0b = tapply(master$normalized_translated,
             master$pos_translated, mean)
m0b

sd0b = tapply(master$normalized_translated,
              master$pos_translated, sd)
sd0b

##subsetting by cue type
adj = subset(master,
             master$pos_cue == "adjective")

noun = subset(master,
              master$pos_cue == "noun")

verb = subset(master,
              master$pos_cue == "verb")

other = subset(master,
               master$pos_cue == "other")              

##adjectives
p1a = percent(adj$pos_feature) ##raw
p1a

p1b = percent(adj$pos_translated) ##root
p1b

m1a = tapply(adj$normalized_feature,
             adj$pos_feature, mean)
m1a

sd1a = tapply(adj$normalized_feature,
              adj$pos_feature, sd)
sd1a

m1b = tapply(adj$normalized_translated,
             adj$pos_translated, mean)
m1b

sd1b = tapply(adj$normalized_translated,
              adj$pos_translated, sd)
sd1b

##nouns
p2a = percent(noun$pos_feature)
p2a

p2b = percent(noun$pos_translated)
p2b

m2a = tapply(noun$normalized_feature,
            noun$pos_feature, mean)
m2a

sd2a = tapply(noun$normalized_feature,
              noun$pos_feature, sd)
sd2a

m2b = tapply(noun$normalized_translated,
             noun$pos_translated, mean)
m2b

sd2b = tapply(noun$normalized_translated,
              noun$pos_translated, sd)
sd2b

##verbs
p3a = percent(verb$pos_feature)
p3a

p3b = percent(verb$pos_translated)
p3b

m3a = tapply(verb$normalized_feature,
             verb$pos_feature, mean)
m3a

sd3a = tapply(verb$normalized_feature,
              verb$pos_feature, sd)
sd3a

m3b = tapply(verb$normalized_translated,
             verb$pos_translated, mean)
m3b

sd3b = tapply(verb$normalized_translated,
              verb$pos_feature, sd)
sd3b

##other
p4a = percent(other$pos_feature)
p4a

p4b = percent(other$pos_translated)
p4b

m4a = tapply(other$normalized_feature,
             other$pos_feature, mean)
m4a

sd4a = tapply(other$normalized_feature,
              other$pos_feature, sd)
sd4a

m4b = tapply(other$normalized_translated,
             other$pos_translated, mean)
m4b

sd4b = tapply(other$normalized_translated,
              other$pos_translated, sd)
sd4b

```

```{r percent-table, echo=FALSE, results='asis'}

####response frequency table####
response.table = matrix(NA, nrow = 20, ncol = 6)

#colnames(response.table) = c("Cue Type", "Feature Type",
                         #"%Raw", "%Root", "$M$ Freq. Raw", "$M$ Freq. Root")

response.table[1, ] = c("Adjective", "Adjective", 38.09, 29.74, "17.84 (16.47)", "30.02 (18.83)")
response.table[2, ] = c(" ", "Noun", 40.02, 46.74, "13.14 (14.96)", "29.71 (19.94)")
response.table[3, ] = c(" ", "Verb", 17.69, 20.72, "8.51 (9.78)", "26.88 (17.27)")
response.table[4, ] = c(" ", "Other", 4.20, 2.80, "15.17 (15.64)", "28.04 (15.54)")
response.table[5, ] = c("Noun", "Adjective", 16.56, 12.07, "15.55 (15.17)", "31.20 (18.17)")
response.table[6, ] = c(" ", "Noun", 60.85, 62.67, "17.21 (17.01)", "33.26 (20.05)")
response.table[7, ] = c(" ", "Verb", 20.80, 23.68, "8.88 (9.73)", "31.01 (17.87)" )
response.table[8, ] = c(" ", "Other", 1.79, 1.58, "17.06 (15.29)", "28.87 (17.14)")
response.table[9, ] = c("Verb", "Adjective", 15.16, 12.27, "13.95 (13.98)", "30.03 (18.28)")
response.table[10, ] = c(" ", "Noun", 42.92, 44.35, "14.59 (14.92)", "29.59 (18.90)")
response.table[11, ] = c(" ", "Verb", 36.92, 39.72, "12.75 (14.85)", "30.43 (19.54)")
response.table[12, ] = c(" ", "Other", 5.00, 3.66, "19.16 (15.95)", "25.59 (19.54)")
response.table[13, ] = c("Other", "Adjective", 20.80, 20.32, "16.61 (17.37)", "31.66 (19.51)")
response.table[14, ] = c(" ", "Noun", 42.74, 39.03, "16.77 (19.41)", "37.28 (25.94)")
response.table[15, ] = c(" ", "Verb", 19.66, 23.93,"7.18 (7.57)", "26.14 (19.38)")
response.table[16, ] = c(" ", "Other", 16.81, 16.71, "22.72 (16.69)", "30.70 (18.48)")
response.table[17, ] = c("Total", "Adjective", 19.74, 14.93, "16.12 (15.57)", "30.75 (18.37)")
response.table[18, ] = c(" ", "Noun", 55.41, 57.81, "16.55 (16.74)", "32.58 (20.09)")
response.table[19, ] = c(" ", "Verb", 22.02, 24.95, "9.50 (10.91)", "30.29 (18.24)")
response.table[20, ] = c(" ", "Other", 2.82, 2.31, "17.76 (15.83)", "28.45 (16.83)")

response.table[ , 3] = apa(as.numeric(response.table[ , 3]), 2)
response.table[ , 4] = apa(as.numeric(response.table[ , 4]), 2)

apa_table.latex(as.data.frame(response.table),
                align = c(rep("l", 2), rep("c", 4)), 
                caption = "Percents and Average Normalized Percent Frequency for Cue-Feature Part of Speech Combinations",
                note = "Raw words indicate original feature listed, while root words indicated translated feature. ",
                escape = FALSE,
               col.names =  c("Cue Type", "Feature Type",
                         "\\% Raw", "\\% Root", "$M$ Freq. Raw", "$M$ Freq. Root"))
```

The top cue-feature combinations the reprocessed and new data collection were then combined with the cue-feature combinations from @McRae2005 and @Vinson2008. We included all of their cue-feature combinations with the cue-feature listed in their supplemental files with the feature in the raw feature column. If features could be translated into root words with affixes, the same procedure as described above was applied. The final file then included the original dataset, cue, feature, translated feature, frequency of the original feature, frequency of the translated feature, sample size, and normalized frequencies for the original and translated feature. This file includes `r nrow(dat1)` cue-original feature combinations, with `r nrow(sub1)` from our dataset, and `r nrow(sub1.reduce)` of which are cue-translated feature combinations. Statistics in Tables \@ref(tab:feature-table) and \@ref(tab:percent-table) only include information from the reprocessed @Buchanan2013 norms and the new cues collected for this project. 

The final data processing step was to code affixes found on the original features. A complete affix list translation file can be found online in our OSF files. Table \@ref(tab:affix-table) displays the list of affix tags, common examples for each type of affix, and the percent of affixes that fell into each category. The percent values are calculated on the overall affix list, as feature words could have up to three different affixes. Generally, affixes were tagged in a one-to-one match, however, special care was taken with numbers and verb tenses. Features like cat*s* would be coded as a number affix, while features like walk*s* would be coded as a third person verb. In the final words file found online, we additionally added forward strength (FSG) and backward strength (BSG) for investigation into association overlap [@Nelson2004]. The last few columns indicate the word list a concept was originally normed in to allow for matching to the original raw files on the OSF page, along with the code for each school and time point of collection.  

```{r affix-calculation, include=FALSE}
####tag examples####
##code below is from what we did last time

library(reshape)

##Creating Affix dataset
affixdata = dat1[,c(2,13:15)]

##Melting dataset
longdata = melt(affixdata,
                id = "cue",
                measured = c("a1", "a2", "a3"))

colnames(longdata) = c("cue", "order", "affix")

summary(longdata)

##Removing zero affixes##
realaffix = subset(longdata, affix != 0)
summary(realaffix)

##Actual percentages for real
final = realaffix
options(scipen = 999)
affixtable = percent(droplevels(final$affix))
affixtable

```

```{r affix-table, echo=FALSE, results='asis'}
####making the affix table####

affix.table = matrix(NA, nrow = 13, ncol = 3)

colnames(affix.table) = c("Affix Tag", "Example",
                         "Percent")

affix.table[1, ] = c("Actions/Processes", "ion, ment, ble, ate, ize", 8.21)
affix.table[2, ] = c("Characteristic", "y, ous, nt, ful, ive, wise", 22.72)
affix.table[3, ] = c("Location", "under, sub, mid, inter", 0.44)
affix.table[4, ] = c("Magnitude", "er, est, over, super, extra", 1.31)
affix.table[5, ] = c("Not", "less, dis, un, non, in , im, ab", 2.76)
affix.table[6, ] = c("Number", "s, uni, bi, tri, semi", 28.31)
affix.table[7, ] = c("Opposites/Wrong", "mis, anti, de", 0.13)
affix.table[8, ] = c("Past Tense", "ed", 8.03)
affix.table[9, ] = c("Person/Object", "er, or, men, person, ess, ist", 7.23)
affix.table[10, ] = c("Present Participle", "ing", 14.03)
affix.table[11, ] = c("Slang", "bros, bike, bbq, diff, h2o", 0.12)
affix.table[12, ] = c("Third Person", "s", 6.16)
affix.table[13, ] = c("Time", "fore, pre, post, re", 0.54)

apa_table.latex(as.data.frame(affix.table),
                align = c(rep("l", 2), rep("c", 1)), 
                caption = "Example of Affix Coding and Percent of Affixes Found",
               col.names =  c("Affix Tag", "Example",
                         "Percent"))
```

This affix processing procedure is a slight departure from our previous work, as we previously argued to keep some morphologically similar features separate if they denoted different concepts. For example, *act* and *actor* were separated because each feature explained a separate component of the cue word (i.e., noun and gender). The original processing in @Buchanan2013 combined features that overlapped in cue sets by 80%. In this reprocessing and update, we translated all words to a root form, and coded these translations, thus, allowing for the exploration of the affect of affixes on semantic feature overlap. Both forms of the feature are provided for flexibility in calculating overlap by using the original feature (raw), the translated feature (root), and the affix overlap by code (affix). Cosine values were calculated for each of these feature sets by using the following formula:

$$
\frac{\sum_{i=1}^{n} A_i \times B_i} {\sqrt{\sum_{i=1}^{n} A_i^2} \times \sqrt{\sum_{i=1}^{n} B_i^2}}
$$

This formula is similar to a dot-product correlation, where $A_i$ and $B_i$ indicate the overlapping feature normalized frequency between cue A and cue B. The *i* subscript denotes the current cue, and when features match, the frequencies are multiplied together and summed across all matches ($\Sigma$). For the denominator, the feature normalized frequency is first squared and summed from *i* to *n* features for cue A and B. The square root of these summation values is then multiplied together. In essence, the numerator calculates the overlap of feature frequency for matching features, while the denominator accounts for the entire feature frequency set for each cue. Cosine values range from 0 (no overlapping features) to 1 (complete overlapping features). With nearly five thousand cue words, just under twenty-five million cue-cue cosine combinations can be calculated. In the datasets presented online, we only included cue-cue combinations with a feature overlap of at least two features, in order to reduce the large quantity of zero and very low cosine values. This procedure additionally allowed for online presentation of the data, as millions of cosines were not feasible for our server. The complete feature list, along with our code to calculate cosine, can be used to obtain values not presented in our data if desired.

## Website

In addition to our OSF page, we present a revamped website for this data at http://www.wordnorms.com/. The single words page includes information about each of the cue words including cue set size, concreteness, word frequency from multiple sources, length, full part of speech, orthographic/phonographic neighborhood, and number of phonemes, syllables, and morphemes. These values were taken from @Nelson2004, @Balota2007, and @Brysbaert2009. A definition of each of these variables is provided along with the minimum, maximum, mean, and standard deviation of numeric values. The table is programmed using Shiny apps [@R-shiny]. Shiny is an *R* package that allows the creation of dynamic graphical user interfaces for interactive web applications. The advantage to using Shiny applications is data manipulation and visualization with the additional bonus of up to date statistics for provided data (i.e., as typos are fixed or data is updated, the web app will display the most recent calculations). In addition to the variable table, users can search and save filtered output using our Shiny search app. With this app, you can filter for specific variable ranges and save the output in a csv or Excel file. The complete data is also provided for download.

On the word pairs page, all information about word-pair statistics can be found. A second variable table is provided with semantic and associative statistics. This dataset includes the cue and target words from this project (cue-cue combinations), the root, raw, and affix cosines described above, as well as the original @Buchanan2013 cosines. Additional semantic information includes Latent Semantic Analysis [LSA; @Landauer1997] and JCN [@Jiang1997] values provided in the @Maki2004 norms, along with forward strength and backward strength (FSG; BSG) from the @Nelson2004 norms for association. The definitions, minimum, maximum, mean, and standard deviations of these values are provided in the app. Again, the search app includes all of these stimuli for cue-cue combinations with two or more features in common, where you can filter this data for experimental stimuli creation. The separation of single and word-pair data (as well as cosine calculation reduction to cues with two features in common) was practical, as the applications run slowly as a factor of the number of rows and columns of data. On each page, we link the data, applications, and source code so that others may use and manipulate our work depending on their data creation or visualization goals.

# Results

An examination of the results of the cue-feature lists indicated that the new data collected was similar to the previous semantic feature production norms. As shown in Table \@ref(tab:feature-table), the new Mechanical Turk data showed roughly the same number of listed features for each cue concept, usually between five to seven features. Table \@ref(tab:percent-table) portrayed that adjective cues generally included other adjectives or nouns as features, while noun cues were predominately described by other nouns. Verb cues included a large feature list of nouns, but then was equally split between adjectives, other verbs, and other categories. Lastly, the other cue types generally elicited nouns and verbs. Normalized percent frequencies were generally between seven and twenty percent when examining the raw words. These words included multiple forms, as the percent increased to around thirty percent when features were translated into their root words. Indeed, nearly half of the `r nrow(sub1)` cue-feature pairs were repeated, as `r nrow(sub1.reduce)` cue-feature pairs were unique when examining translated features. 

`r nrow(realaffix)` affix values were found, which arose from `r length(unique(realaffix$cue))` of the 4437 cue concepts. `r table(realaffix$order)["a1"]` first affixes were found, with `r table(realaffix$order)["a2"]` second place affixes, and `r table(realaffix$order)["a3"]` third place affixes. Table \@ref(tab:affix-table) shows the distribution of these affix values. Generally, numbers were the largest category of affixes demonstrating that participants often indicated the quantity of the feature when describing the cue word. The second largest affix category was characteristics which often indicated the switch to or from a noun form of the feature word (i.e., *angry* to *anger*). Verb tenses (past tense, present participle, and third person) comprised a large set of affixes indicating the type of concept or when a concept might be doing an action for a cue. Persons and objects were also indicated about 7% of the time, while actions and processes of the cue were mentioned about 8% of the time. 

## Divergent Validity

```{r divergent-table, echo=FALSE, results='asis'}

####fsg/bsg table will go here####
##can use sub1 dataset from other tables
##Percent goes below

fsg.table = as.data.frame(matrix(NA, nrow = 5, ncol = 6))
colnames(fsg.table) = c("stim", "overlap", "mean", "sd", "min", "max")

percentNOTmiss = function(x){ sum(is.na(x) == FALSE)/length(x) *100 }

#cue translated FSG but remove the duplicates due to the translation
fsgtest = unique(dat1[ , c("FSG", "pos_cue", "cue", "translated")])
fsgtest$pos_cue = factor(fsgtest$pos_cue,
                         levels = c("adjective", "noun", "verb", "other"),
                         labels = c("Adjective", "Noun", "Verb", "Other"))

overlap = tapply(fsgtest$FSG, fsgtest$pos_cue, percentNOTmiss)
meanfsg = tapply(fsgtest$FSG, fsgtest$pos_cue, mean, na.rm = T)
sdfsg = tapply(fsgtest$FSG, fsgtest$pos_cue, sd, na.rm = T)
minfsg = tapply(fsgtest$FSG, fsgtest$pos_cue, min, na.rm = T)
maxfsg = tapply(fsgtest$FSG, fsgtest$pos_cue, max, na.rm = T)

#overlap;meanfsg;sdfsg;minfsg;maxfsg

fsg.table$stim = c(levels(fsgtest$pos_cue), "Total")
fsg.table$overlap = apa(c(overlap, percentNOTmiss(fsgtest$FSG)),2)
fsg.table$mean = apa(c(meanfsg, mean(fsgtest$FSG, na.rm = T)), 2, F)
fsg.table$sd = apa(c(sdfsg, sd(fsgtest$FSG, na.rm = T)), 2, F)
fsg.table$min = apa(c(minfsg, min(fsgtest$FSG, na.rm = T)), 2, F)
fsg.table$max = apa(c(maxfsg, max(fsgtest$FSG, na.rm = T)), 2, F)
  
apa_table.latex(fsg.table,
                align = c(rep("l", 1), rep("c", 5)), 
                caption = "Percent and Mean Overlap to the Free Association Norms",
                escape = FALSE,
               col.names =  c(" ", "\\% Overlap", "$M$ FSG", "$SD$ FSG", "Min", "Max"))

```

When collecting semantic feature production norms, there can be a concern that the information produced will simply mimic the free association norms, and thus, be a representation of association (context) rather than semanticity (meaning). Table \@ref(tab:divergent-table) portrays the overlap with the @Nelson2004 norms. The percent of time a cue-feature combination was present in the free association norms was calculated, along with the average FSG for those overlapping pairs. These values were calculated on the complete dataset with the @McRae2005 and @Vinson2008 norms, as we are presenting them as a combined dataset, on the translated cue-feature set only. The overall overlap between the database cue-feature sets and the free association cue-target sets was approximately 37%, ranging from 32% for verbs and nearly 52% for adjectives. Similar to our previous results, the range of the FSG was large (.01 - .94), however, the average FSG was low for overlapping pairs, *M* = .11 (*SD* = .14). These results indicated that while it will always be difficult to separate association and meaning, the dataset presented here represents a low association when examining overlapping values, and more than 60% of the data is completely separate from the free association norms. The limitation to this finding is the removal of idiosyncratic responses from the @Nelson2004 norms, but even if these were to be included in some form, the average FSG would still be quite low when comparing cue-feature lists to cue-target lists. 

## Convergent Validity

```{r convergent, include = FALSE}
cosmatch = read.csv("cosine_matches.csv")
b = subset(cosmatch, where1 == "b") 
m = subset(cosmatch, where1 == "m")  

meanroot = tapply(cosmatch$V3, list(cosmatch$where1, cosmatch$where2), mean)
meanraw = tapply(cosmatch$V4, list(cosmatch$where1, cosmatch$where2), mean)
meanaffix = tapply(cosmatch$V5, list(cosmatch$where1, cosmatch$where2), mean)  

sdroot = tapply(cosmatch$V3, list(cosmatch$where1, cosmatch$where2), sd)
sdraw = tapply(cosmatch$V4, list(cosmatch$where1, cosmatch$where2), sd)
sdaffix = tapply(cosmatch$V5, list(cosmatch$where1, cosmatch$where2), sd)  
```

To examine the validity of cosine values, we calculated the average cosine score between the new processing of the data for each of the three feature production norms used in this project. Overlapping cues in each of three database sets were found (*n* = `r length(unique(cosmatch$cue))`), and the average cosine between their feature sets was examined. @Buchanan2013 and the new dataset are listed the subscript B, while @McRae2005 is M and V for @Vinson2008. For root cosine values, we found high overlap between all three datasets: $M_{BM}$ = `r apa(meanroot[1], 2, F)` (*SD* = `r apa(sdroot[1], 2, F)`), $M_{BV}$ = `r apa(meanroot[3], 2, F)` (*SD* = `r apa(sdroot[3], 2, F)`), and $M_{MV}$ = `r apa(meanroot[4], 2, F)` (*SD* = `r apa(sdroot[4], 2, F)`). The raw cosine values also overlapped well, even though the @McRae2005 and @Vinson2008 datasets were already mostly preprocessed for word stems: $M_{BM}$ = `r apa(meanraw[1], 2, F)` (*SD* = `r apa(sdraw[1], 2, F)`), $M_{BV}$ = `r apa(meanraw[3], 2, F)` (*SD* = `r apa(sdraw[3], 2, F)`), and $M_{MV}$ = `r apa(meanraw[4], 2, F)` (*SD* = `r apa(sdraw[4], 2, F)`). Last, the affix cosines overlapped similarly between BM datasets, $M_{BM}$ = `r apa(meanaffix[1], 2, F)` (*SD* = `r apa(sdaffix[1], 2, F)`), but did not overlap with the V datasets: $M_{BV}$ = `r apa(meanaffix[3], 2, F)` (*SD* = `r apa(sdaffix[3], 2, F)`), and $M_{MV}$ = `r apa(meanaffix[4], 2, F)` (*SD* = `r apa(sdaffix[4], 2, F)`), likely due to V dataset preprocessing.

The correlation between root, raw, affix, previously found cosine, LSA, and JCN were calculated to examine convergent validity. As shown in Table \@ref(tab:correlation-table), the intercorrelations between the cosine measures are high, especially between our previous work and this dataset. JCN is backwards coded, as zero values indicate close semantic neighbors (low dictionary distance) and high values indicate low semantic relation. The small negative correlations replicated previous findings [@Buchanan2013]. LSA values showed small positive correlations with cosine values, indicating some overlap with thematic information and semantic feature overlap [@Maki2008]. These correlations were slightly different than our previous publication, likely because we restricted this cosine set to values with at least two features in common. LSA and JCN correlations were lower than LSA-COS and JCN-COS, but these values indicated that themes and dictionary distance were similarly related to feature overlap. 

```{r correlations, eval=FALSE, eval=FALSE, include=FALSE}

####Correlation table goes here####

dat.cor = averaged_cosine ##imported via file viewer, acc.csv

dat.cor$jcn = as.numeric(dat.cor$jcn)
dat.cor$fsg = as.numeric(dat.cor$fsg)
dat.cor$oldcos = as.numeric(dat.cor$oldcos)
dat.cor$lsa = as.numeric(dat.cor$lsa)
dat.cor$bsg = as.numeric(dat.cor$bsg)

cor(dat.cor[ , -c(1:2)], use = "pairwise.complete.obs")

```

```{r correlation-table, echo=FALSE, results='asis'}

cor.table = matrix(NA, nrow = 8, ncol = 9)

colnames(cor.table) = c(" ", "Root", "Raw", "Affix", "Previous COS", "JCN", "LSA", "FSG", "BSG")

cor.table[1, ] = c("Root", 1, " " , " " , " " , " " , " ", " ", " ")
cor.table[2, ] = c("Raw", .93, 1, " ", " ", " ", " ", " ", " ")
cor.table[3, ] = c("Affix", .50, .53, 1, " ", " ", " ", " ", " ")
cor.table[4, ] = c("Previous COS", .94, .91, .49, 1, " ", " ", " ", " ")
cor.table[5, ] = c("JCN", -.18, -.22, -.17, -.22, 1, " ", " ", " ")
cor.table[6, ] = c("LSA", .18, .15, .10, .21, -.06, 1, " ", " ")
cor.table[7, ] = c("FSG", .06, .04, .08, .10, -.15, .24, 1, " ")
cor.table[8, ] = c("BSG", .14, .15, .17, .18, -.18, .26, .31, 1)

cor.table[2:8 , 2] = apa(as.numeric(cor.table[2:8 , 2]), 2, F)
cor.table[3:8 , 3] = apa(as.numeric(cor.table[3:8 , 3]), 2, F)
cor.table[4:8 , 4] = apa(as.numeric(cor.table[4:8 , 4]), 2, F)
cor.table[5:8 , 5] = apa(as.numeric(cor.table[5:8 , 5]), 2, F)
cor.table[6:8 , 6] = apa(as.numeric(cor.table[6:8 , 6]), 2, F)
cor.table[7:8 , 7] = apa(as.numeric(cor.table[7:8 , 7]), 2, F)
cor.table[8 , 8] = apa(as.numeric(cor.table[8 , 8]), 2, F)


apa_table(as.data.frame(cor.table),
                align = c(rep("l", 1), rep("c", 8)), 
                caption = "Correlations between Semantic, Associative, and Thematic Variables",
               col.names =  c(" ", "Root", "Raw", "Affix", "Previous COS", "JCN", "LSA", "FSG", "BSG"))
```

## Relation to Semantic Priming

```{r ldt-table, echo=FALSE, results='asis'}
##load data sets
faldt.dat = read.csv("FALDT.csv")
fan.dat = read.csv("FAN.csv")
oaldt.dat = read.csv("OALDT.csv")
oan.dat = read.csv("OAN.csv")

#figure out the correlations of the datasets
#want columns 18, 19, 21, 29, 32, 49, 50, 51, 52, 86, 87, 88, 90, 91, 92

faldt.cor = cor(faldt.dat[ , c(18, 19, 21, 29, 32, 49, 50, 51, 83, 86, 87, 88, 90, 91, 92)], use = "pairwise.complete.obs")

fan.cor = cor(fan.dat[ , c(18, 19, 21, 29, 32, 49, 50, 51, 83, 86, 87, 88, 90, 91, 92)], use = "pairwise.complete.obs")

oaldt.cor = cor(oaldt.dat[ , c(18, 19, 21, 29, 32, 49, 50, 51, 83, 86, 87, 88, 90, 91, 92)], use = "pairwise.complete.obs")

oan.cor = cor(oan.dat[ , c(18, 19, 21, 29, 32, 49, 50, 51, 83, 86, 87, 88, 90, 91, 92)], use = "pairwise.complete.obs")

cor.table = as.data.frame(matrix(NA, nrow = 13, ncol = 5))
colnames(cor.table) = c("Variable", "FA-LDT 200", "FA-LDT 1200", "OA-LDT 200", "OA-LDT 1200")

cor.table[ , 1] = c("Root COS", "Raw COS", "Affix COS", 
                    "Target CSS", "Target Raw FSS", "Target Root FSS", 
                    "Cue CSS", "Cue Raw FSS", "Cue Root FSS", 
                    "FSG", "BSG", "LSA", "JCN")
cor.table[ , 2] = apa(faldt.cor[c(6,7,8,10:15,1,2,3,9) , 4],2,F)
cor.table[ , 3] = apa(faldt.cor[c(6,7,8,10:15,1,2,3,9) , 5],2,F)
cor.table[ , 4] = apa(oaldt.cor[c(6,7,8,10:15,1,2,3,9) , 4],2,F)
cor.table[ , 5] = apa(oaldt.cor[c(6,7,8,10:15,1,2,3,9) , 5],2,F)

##reorder a bit
cor.table = cor.table[ c(1:3, 6:4, 9:7, 10:13), ]

apa_table(as.data.frame(cor.table),
          align = c(rep("l", 1), rep("c", 4)), 
          caption = "LDT Response Latencies Correlation with Semantic and Associative Variables",
          note = "Missing values excluded pairwise for JCN. FA: first associate, OA: other associate, CSS: cue set size, and FSS: feature set size.")
```

```{r name-table, echo=FALSE, results='asis'}

cor.table = as.data.frame(matrix(NA, nrow = 13, ncol = 5))
colnames(cor.table) = c("Variable", "FA-Name 200", "FA-Name 1200", "OA-Name 200", "OA-Name 1200")

cor.table[ , 1] = c("Root COS", "Raw COS", "Affix COS", 
                    "Target CSS", "Target Raw FSS", "Target Root FSS", 
                    "Cue CSS", "Cue Raw FSS", "Cue Root FSS", 
                    "FSG", "BSG", "LSA", "JCN")
cor.table[ , 2] = apa(fan.cor[c(6,7,8,10:15,1,2,3,9) , 4],2,F)
cor.table[ , 3] = apa(fan.cor[c(6,7,8,10:15,1,2,3,9) , 5],2,F)
cor.table[ , 4] = apa(oan.cor[c(6,7,8,10:15,1,2,3,9) , 4],2,F)
cor.table[ , 5] = apa(oan.cor[c(6,7,8,10:15,1,2,3,9) , 5],2,F)

##reorder a bit
cor.table = cor.table[ c(1:3, 6:4, 9:7, 10:13), ]

apa_table(as.data.frame(cor.table),
          align = c(rep("l", 1), rep("c", 4)), 
          caption = "Naming Response Latencies Correlation with Semantic and Associative Variables",
          note = "Missing values excluded pairwise for JCN. FA: first associate, OA: other associate, CSS: cue set size, and FSS: feature set size.")
```

As a second examination of convergent validity, the correlation between values calculated from these norms and the *Z* priming values from the Semantic Priming Project were examined. The Semantic Priming Project includes lexical decision and naming response latencies for priming at 200 and 1200 ms stimulus onset asynchronies (SOA). In these experiments, participants were shown cue-target words that were either the first associate of a concept or an other associate (second response or higher in the @Nelson2004 norms). The response latency of the target word was subtracted from the non-primed lexical decision or naming time using the English Lexicon Project as the baseline expected response latencies for concepts. Therefore, each target item received four (two SOAs by two tasks lexical decision or naming) priming times, and we selected the *Z*-scored priming from the dataset to correlate with our data. In addition to root, raw, and affix cosine, we additionally calculated feature set size for the cue and target of the primed pairs. Feature set size is the number of features listed by participants when creating the norms for that concept. Because of the nature of our norms, we calculated both feature set size for the raw, untranslated features, as well as the translated features. The average feature set sizes for our dataset can be found in Table \@ref(tab:feature-table). The last variable included was cosine set size which was defined as the number of other concepts each cue or target was nonzero paired with in the cosine values. Feature set size indicates the number of features listed for each cue or target, while cosine set size indicates the number of other semantically related concepts for each cue or target.   

Tables \@ref(tab:ldt-table) and \@ref(tab:name-table) display the correlations between the new semantic variables described above, as well as FSG, BSG, LSA, and JCN for reference. For lexical decision priming, we found small correlations between the root and raw cosine values and priming, with the largest for first associates in the 200 ms condition. The correlations decreased for the 1200 ms condition and the other associate SOAs. These two variables are highly correlated, therefore, it is not surprising that they have similar correlations with priming. Affix cosine also was slightly related to priming, especially for first associates in the 200 ms condition. Most of the cue and feature set sizes were not related to priming, showing correlations close to zero in most instances. Cue set size for the cue word was somewhat related to 200 ms priming, along with raw cue feature set size. These correlations are small, but they are comparable or greater than the correlations for association and other measures of semantic or thematic relatedness. For naming, the results are less consistent. Cosine values are related to 1200 ms naming in first and other associates, and none of the feature or cue set sizes showed any relationship with priming. Again, we see that many of the other associative and semantic variables correspondingly do not correlate with priming. In both naming and lexical decision priming, BSG has a small but consistent relationship with priming, which may indicate the processing of the target back to the cue. LSA was also a small predictor of priming across conditions.   

# Discussion

This research project focused on expanding the availability of English semantic feature overlap norms, in an effort to provide more coverage of concepts that occur in other large database projects like the Semantic Priming and English Lexicon Projects. The number and breadth of linguistic variables and normed databases has increased over the years, however, researchers can still be limited by the concept overlap between them. Projects like the Small World of Words provide newly expanded datasets for association norms, and our work helps fill the voids for corresponding semantic norms. To provide the largest dataset of similar data, we combined the newly collected data with previous work by using @Buchanan2013, @McRae2005, and @Vinson2008 together. These norms were reprocessed from previous work to explore the impact of feature coding for feature overlap. As shown in the correlation between root and raw cosines, the parsing of words to root form created very similar results across other variables. This finding does not imply that these cosine values are the same, as root cosines were larger than their corresponding raw cosine. It does, however, imply that the cue-feature coding can produce similar results in raw or translated format. 

Of particular interest was the information that is often lost when translating raw features back to a root word. One surprising result in this study was the sheer number of affixes present on each cue word. With these values, we believe we have captured some of the nuance that is often discarded in this type of research. Affix cosines were less related to their feature root and raw counterparts, but also showed small correlations with semantic priming. Potentially, affix overlap can be used to add small, but meaningful predictive value to related semantic phenomena. Further investigation into the compound prediction of these variables is warranted to fully explore how these, and other lexical variables, may be used to understand semantic priming. An examination of the cosine values from the Semantic Priming Project cue-target set indicates that these values were low, with many zeros. This restriction of range could explain the small correlations with priming, along with the understanding that semantic priming itself can be exceedingly variable and small across items. 

We encourage readers to use the corresponding website associated with these norms to download the data, explore the Shiny apps, and use the options provided for controlled experimental stimuli creation. We previously documented the limitations of feature production norms that rely on on single word instances as their features (i.e., *four* and *legs*), rather than combined phrase sets. One limitation, potentially, is the inability to create fine distinctions between cues; however, the small feature set sizes imply that the granulation of features is large, since many distinguishing features are often never listed in these tasks. For instance, *dogs* are living creatures, but *has lungs* or *has skin* would usually not be listed during a feature production task, and thus, feature sets should not be considered a complete snapshot of mental representation [@Rogers2004]. The previous data and other norms were purposely combined in the recoded format, so that researchers could use the entire set of available norms which increases comparability across datasets. Given the strong correlation between databases, we suspect that using single word features does not reduce their reliability and validity. 

One other important limitation of the instructions in this study is that multiple senses of concepts were not distinguished. We did not wish to prime participants for specific senses to capture the features for multiple senses of a concept, however, this procedure could lead to lower cosine values for concepts that might intuitively seem very related. The feature production lists could be used to sort senses and recalculate overlap values, but it is likely that feature information is correspondingly mixed or sorted into small sublists in memory as well. The addition of the coded affix information may help capture some of those sense differences, as well as the some of the spatial and relational features that are not traditionally captured by simple feature production. For example, by understanding the numbers or actors affixes, we may gain more information about semanticity that is often regarded as something to disregard in data processing. 

\newpage

# References
```{r create_r-references}
r_refs(file = "r-references.bib")
```

\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}
