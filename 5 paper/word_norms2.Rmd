---
title             : "English Semantic Feature Production Norms: An Extended Database of 4,437 Concepts"
shorttitle        : "Semantic Norms"

author: 
  - name          : "Erin M. Buchanan"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "901 S. National Ave, Springfield, MO 65897"
    email         : "erinbuchanan@missouristate.edu"
  - name          : "K. D. Valentine"
    affiliation   : "2"
  - name          : "Nicholas P. Maxwell"
    affiliation   : "1"

affiliation:
  - id            : "1"
    institution   : "Missouri State University"
  - id            : "2"
    institution   : "University of Missouri"

author_note: |
  Erin M. Buchanan is an Associate Professor of Quantitative Psychology at Missouri State University. K. D. Valentine is a Ph.D. candidate at the University of Missouri. Nicholas P. Maxwell is a Masters' candidate at Missouri State University.
  
  We would like to thank Keith Hutchison and David Balota for their contributions to this project, including the funds to secure Mechanical Turk participants. 

abstract: |
  The largest limiting factor in understanding memory and language networks is often the availability of normed stimuli to use and explore in experimental studies. In this study, we expand on three previous semantic feature overlap norms to over 4,000 cue stimuli ranging from nouns, verbs, adjectives, and other parts of speech. Participants in the norming study were asked to provide feature components of each cue stimuli, which were combined with the previous research using semantic feature production procedures. In addition to expanding previous research, this project explores different semantic overlap measurements by coding each word feature listed by root and affixes to determine different strengths of feature overlap. All information is provided in a searchable database for easy access and utilization for future researchers when designing experiments. The final database of cue-target pairs was paired with the Semantic Priming Project to examine the predictive ability of semanticity on semantic priming in tandem with other psycholinguistic variables, such as association, thematics, and frequency. Target concept frequency was the largest predictor of semantic priming, follow by thematics and association. Root word cosine was predictive of semantic priming, even after adjusting for the previously mentioned psycholinguistic variables.
  
keywords          : "semantics, word norms, database, psycholinguistics"

bibliography      : ["r-references.bib", "wn2.bib"]

figsintext        : no
figurelist        : no
tablelist         : no
footnotelist      : no
lineno            : yes

lang              : "english"
class             : "man"
output            : papaja::apa6_pdf
replace_ampersands: yes
csl               : apa6.csl
---

```{r load_packages, include = FALSE}
library("papaja")
library(MOTE)
```

Semantic representations are the focus of a large area 

# Method

## Participants

Participants in the newly collected stimuli were gathered from Amazon's Mechanical Turk, which is a large, diverse participant pool wherein users can complete surveys for small sums of money [@Buhrmester2011]. Answers can be screened for errors, and incorrect or incomplete surveys can be rejected or discarded without payment. Each participant was paid five cents for a survey, and they could complete multiple Human Intelligence Tasks or HITS. Each HIT included five concepts, and HITS would remain active until *n* = 30 valid survey answers were collected. HITS were usually rejected if they included copied definitions from Wikipedia, "I don't know", or writing a paragraph about the concept. These answers were discarded, as described below. Table \@ref(tab:part-table) includes the sample sizes from the new study (Mechanical Turk 2), as well as the sample sizes from the previous study, as described in @Buchanan2013. 

```{r part-table, echo=FALSE, results='asis'}
####partcipant table####
participant.table2 = matrix(NA, nrow = 5, ncol = 4)

colnames(participant.table2) = c("Institution", "Total Participants",
                         "Concepts", "Mean $N$")

participant.table2[1, ] = c("University of Mississippi", 749, 658, 67.8)
participant.table2[2, ] = c("Missouri State University", 1420, 720, 71.4)
participant.table2[3, ] = c("Montana State University", 127, 120, 63.5)
participant.table2[4, ] = c("Mechanical Turk 1", 571, 310, 60)
participant.table2[5, ] = c("Mechanical Turk 2", 198, 1914, 30)         

apa_table.latex(as.data.frame(participant.table2),
                align = c(rep("l", 1), rep("c", 3)), 
                caption = "",
                note = "",
                escape = FALSE,
                col.names = c("Institution", "Total Participants",
                         "Concepts", "Mean $N$"))
```

## Materials

The purpose of this second norming set was to expand the @Buchanan2013 norms to include all concepts from the Semantic Priming Project [@Hutchison2010]. Therefore, these concepts were the target of the project. The original concept set was selected primarily from the @Nelson2004 database, with small overlaps in the @McRae2005 and @Vinson2008 database sets for convergent validity. In the Semantic Priming Project, cue-target pairs were shown to participants to examine naming and lexical decision time priming across related and unrelated pairs. The related pairs included first associate (most common response to a cue) and other associates (second or greater common responses to cues) as their target words. The original publication of concepts included the cue words from the Semantic Priming Project, while this project expanded to include missed cue words and all target words. The addition of these concepts allowed for complete overlap between the Semantic Priming Project and the feature production norms. 

```{r calculate-frequency, include=FALSE}

dat1 = read.csv("final words 2017.csv")
sub1 = subset(dat1,
              dat1$where == "b")

####stimuli table####

#we want the number of features for each concept
#reduce down to just the translated data
sub1 = sub1[ , c("cue", "translated", "pos_cue", "school_code")]
sub1 = unique(sub1)
sub1$cue = droplevels(sub1$cue)

#for each cue calculate the number of features listed
nofeatures = as.data.frame(table(sub1$cue))
colnames(nofeatures)[1] = "cue"

#add in the school code and pos_cue
finalfreq = unique(merge(nofeatures, sub1[ , c("cue", "pos_cue", "school_code")], by = "cue"))

#now calculate statistics - means by school
with(finalfreq, tapply(Freq, list(pos_cue, school_code), mean))
with(finalfreq, tapply(Freq, list(pos_cue, school_code), sd))

##mean frequency per school
with(finalfreq, tapply(Freq, list(school_code), mean))
with(finalfreq, tapply(Freq, list(school_code), sd))

##mean frequency by part of speech
with(finalfreq, tapply(Freq, list(pos_cue), mean))
with(finalfreq, tapply(Freq, list(pos_cue), sd))

##total total 
mean(finalfreq$Freq); sd(finalfreq$Freq)

partofspeech = unique(dat1[ , c("cue", "pos_cue")])
POS = apa(table(partofspeech$pos_cue)/nrow(partofspeech)*100,1)
```

Concepts were labeled by part of speech using the English Lexicon Project [@Balota2007], the free association norms, and Google's define search when necessary. When labelling these words, we used the most common part of speech to categorize concepts. This choice was predominately for simplicity of categorization, however, the participants were shown concepts without the suggestion of which sense to use for the word. Therefore, multiple senses are embedded into the feature production norms, while the database is labeled with single parts of speech. The other parts of speech can be found in the English Lexicon Project or multiple other databases. This dataset was combined with @McRae2004 and @Vinson2008 feature production norms, which was a combined total of `r nrow(partofspeech)` concepts. `r POS["noun"]`% of concepts were nouns, `r POS["adjective"]`% adjectives, `r POS["verb"]`% verbs, and `r POS["other"]`% were other forms of speech, such as adverbs and conjunctions.

## Procedure

Each HIT was kept to five concepts, and usual survey response times were five to seven minutes. Each HIT was open until thirty participants had successfully completed the HIT and were paid the five cents for the HIT. The survey instructions were copied from @McRae2005's Appendix B, which were also used in the previous publication of these norms. Because the @McRae2005 data was collected on paper, we modified these instructions slightly. The original lines to write in responses were changed to an online textbox response window. The detailed instructions additionally no longer contained information about how a participant should only consider the noun of the target concept, as the words in our study included multiple forms of speech and senses. Participants were encouraged to list the properties or features of each concept in the following areas: physical (looks, sounds, and feels), functional (uses), and categorical (belongings). The same examples used previously (*duck, cucumber, stove*) were included to aid in task understanding and completion. Participants signed up for the HITS through Amazon's Mechanical Turk website and completed the study within the Mechanical Turk framework. Approved HITs were compensated through the Mechanical Turk system. All answers were then combined into a larger dataset. 

## Data Processing

The entire dataset, at each processing stage described here, can be found at: https://osf.io/cjyzw/. On our OSF page, we have included a detailed processing guide on how concepts were (re)examined for this publication. This paper was written with *R* markdown and *papaja* [@R-papaja]. The markdown document allows an interested reader to view the scripts that created the article in line with the written text. However, the processing of the text documents was performed on the raw files, and therefore, we have included the processing guide for transparency of each stage. 

First, each concept was separated into an individual text file that is included as the "raw" data online. Each of these files was then spell checked and corrected when the participant answer was obviously a typo. As noted early, participants often tried to cut and paste Wikipedia or other online dictionary sources into the their answers to complete surveys quickly with minimal effort. These entries were easily found by the formatting of the webpage that was included in their answer. These answers were then discarded from the concept individual text files. Next, each concept was processed for feature frequency. In this stage, the raw frequency counts of each cue-feature combination were calculated and put together into one large file. Cue-cue combinations were discarded, as participants might write "a zebra is a horse" when asked to define *zebra*. English stop words such as *the, an, of* were then discarded, as well as terms that were often used as part of a definition (*like, means, describes*). 

To create the final root cosine values, we then created a "translated" column for each feature listed. This column indicated the root word for each feature, along with the affixes that were used in the original feature. For example, the original feature *cats* would be translated to *cat* and *s*, wherein *cat* would be the translated feature and the *s* would be the affix code. Multiple affix codes were often needed for features, as *beautifully* would have been translated to *beauty*, *ful*, and *ly*. Often, the noun version of the feature would be used for the translation or the most common part of speech for each feature would be recorded. The sample size for the cue was added to this dataset, as the sample sizes varied across experiment time, as shown in Table \@ref(tab:part-table). Therefore, instead of using raw feature frequency, we normalized each count into a percent of participants that included that feature with each cue. At this stage, the data was reduced to cue-feature combinations that were listed by at least 16% of participants (matching @McRae2005's procedure) or were in the top five features listed for that cue. This calculation was performed on the translated normalized feature percent. For example, *beauty* may have been listed as *beauty, beautiful, beautifully*, and this feature would have been listed three times in the dataset for the original cue. The *frequency_feature* column indicates the frequency of the original, unedited feature, while the *frequency_translated* includes all combinations of *beauty* into one overall feature. Because non-nouns can be more difficult to create a feature list for, we included the top five descriptors in addition to the 16% listed criteria, to ensure that each concept included at least five features. Table \@ref(tab:feature-table) indicates the percent f

number of features listed for each collection site/time point by part of speech for the cue word. 

```{r feature-table, echo=FALSE, results='asis'}
##table
stim.table = matrix(NA, nrow = 6, ncol = 6)

colnames(stim.table) = c("Institution", "Adjective",
                         "Noun", "Verb", "Other", "Total")

stim.table[1, ] = c("University of Mississippi", "10.97 (12.48)", "10.40 (13.08)",  "5.28 (6.71)", "10.01 (10.16)", "9.45 (12.02)")
stim.table[2, ] = c("Missouri State University", "10.41 (11.57)", "10.71 (13.48)", "5.87 (6.46)", "10.11 (11.74)", "9.52 (11.94)")
stim.table[3, ] = c("Montana State University", "10.95 (11.58)", "10.66 (12.13)", "5.89 (7.86)", "11.27 (10.64)", "9.67 (11.34)")
stim.table[4, ] = c("Mechanical Turk 1", "9.00 (8.92)", "9.73 (11.35)", "5.17 (6.64)", "11.30 (10.41)", "8.65 (10.17)")
stim.table[5, ] = c("Mechanical Turk 2", "5.23 (4.85)", "5.71 (5.15)", "3.36 (3.50)", "5.87 (4.84)", "5.02 (4.81)")         
stim.table[6, ] = c("Total", "8.32 (9.62)", "8.19 (10.07)", "4.55 (5.91)", "8.31 (8.52)", "7.42 (9.30)")

apa_table.latex(as.data.frame(stim.table),
                align = c(rep("l", 1), rep("c", 5)), 
                caption = "",
                note = "",
                escape = FALSE,
                col.names =  c("Institution", "Adjective",
                         "Noun", "Verb", "Other", "Total"))
```


The final file then included the original dataset, cue, feature, 




# Results



```{r, include=FALSE}

####Set up for response frequency table####
##this code came from what we did last time

library(memisc) ##percent function comes from memisc library

##total
master = sub1

p0a = percent(master$pos_feature) #raw
p0a

p0b = percent(master$pos_translated) ##root
p0b

m0a = tapply(master$normalized_feature,
             master$pos_feature, mean)
m0a

sd0a = tapply(master$normalized_feature,
              master$pos_feature, sd)
sd0a

m0b = tapply(master$normalized_translated,
             master$pos_translated, mean)
m0b

sd0b = tapply(master$normalized_translated,
              master$pos_translated, sd)
sd0b

##subsetting by cue type
adj = subset(master,
             master$pos_cue == "adjective")

noun = subset(master,
              master$pos_cue == "noun")

verb = subset(master,
              master$pos_cue == "verb")

other = subset(master,
               master$pos_cue == "other")              

##adjectives
p1a = percent(adj$pos_feature) ##raw
p1a

p1b = percent(adj$pos_translated) ##root
p1b

m1a = tapply(adj$normalized_feature,
             adj$pos_feature, mean)
m1a

sd1a = tapply(adj$normalized_feature,
              adj$pos_feature, sd)
sd1a

m1b = tapply(adj$normalized_translated,
             adj$pos_translated, mean)
m1b

sd1b = tapply(adj$normalized_translated,
              adj$pos_translated, sd)
sd1b

##nouns
p2a = percent(noun$pos_feature)
p2a

p2b = percent(noun$pos_translated)
p2b

m2a = tapply(noun$normalized_feature,
            noun$pos_feature, mean)
m2a

sd2a = tapply(noun$normalized_feature,
              noun$pos_feature, sd)
sd2a

m2b = tapply(noun$normalized_translated,
             noun$pos_translated, mean)
m2b

sd2b = tapply(noun$normalized_translated,
              noun$pos_translated, sd)
sd2b

##verbs
p3a = percent(verb$pos_feature)
p3a

p3b = percent(verb$pos_translated)
p3b

m3a = tapply(verb$normalized_feature,
             verb$pos_feature, mean)
m3a

sd3a = tapply(verb$normalized_feature,
              verb$pos_feature, sd)
sd3a

m3b = tapply(verb$normalized_translated,
             verb$pos_translated, mean)
m3b

sd3b = tapply(verb$normalized_translated,
              verb$pos_feature, sd)
sd3b

##other
p4a = percent(other$pos_feature)
p4a

p4b = percent(other$pos_translated)
p4b

m4a = tapply(other$normalized_feature,
             other$pos_feature, mean)
m4a

sd4a = tapply(other$normalized_feature,
              other$pos_feature, sd)
sd4a

m4b = tapply(other$normalized_translated,
             other$pos_translated, mean)
m4b

sd4b = tapply(other$normalized_translated,
              other$pos_translated, sd)
sd4b

```

```{r, echo=FALSE, results='asis'}

####response frequency table####
response.table = matrix(NA, nrow = 20, ncol = 6)

colnames(response.table) = c("Cue Type", "Feature Type",
                         "%Raw", "%Root", "$M$ Freq. Raw", "$M$ Freq. Root")

response.table[1, ] = c("Adjective", "Adjective", 38.09, 29.74, "17.84(16.47)", "30.02(18.83)")
response.table[2, ] = c(" ", "Noun", 40.02, 46.74, "13.14(14.96)", "29.71(19.94)")
response.table[3, ] = c(" ", "Verb", 17.69, 20.72, "8.51(9.78)", "26.88(17.27)")
response.table[4, ] = c(" ", "Other", 4.20, 2.80, "15.17(15.64)", "28.04(15.54)")
response.table[5, ] = c("Noun", "Adjective", 16.56, 12.07, "15.55(15.17", "31.20(18.17)")
response.table[6, ] = c(" ", "Noun", 60.85, 62.67, "17.21(17.01)", "33.26(20.05)")
response.table[7, ] = c(" ", "Verb", 20.80, 23.68, "8.88(9.73)", "31.01(17.87)" )
response.table[8, ] = c(" ", "Other", 1.79, 1.58, "17.06(15.29)", "28.87(17.14)")
response.table[9, ] = c("Verb", "Adjective", 15.16, 12.27, "13.95(13.98)", "30.03(18.28")
response.table[10, ] = c(" ", "Noun", 42.92, 44.35, "14.59(14.92)", "29.59(18.90")
response.table[11, ] = c(" ", "Verb", 36.92, 39.72, "12.75(14.85)", "30.43(19.54)")
response.table[12, ] = c(" ", "Other", 5.00, 3.66, "19.16(15.95)", "25.59(19.54)")
response.table[13, ] = c("Other", "Adjective", 20.80, 20.32, "16.61(17.37)", "31.66(19.51)")
response.table[14, ] = c(" ", "Noun", 42.74, 39.03, "16.77(19.41)", "37.28(25.94")
response.table[15, ] = c(" ", "Verb", 19.66, 23.93,"7.18(7.57)", "26.14(19.38")
response.table[16, ] = c(" ", "Other", 16.81, 16.71, "22.72(16.69)", "30.70(18.48)")
response.table[17, ] = c("Total", "Adjective", 19.74, 14.93, "16.12(15.57)", "30.75(18.37)")
response.table[18, ] = c(" ", "Noun", 55.41, 57.81, "16.55(16.74)", "32.58(20.09)")
response.table[19, ] = c(" ", "Verb", 22.02, 24.95, "9.50(10.91)", "30.29(18.24)")
response.table[20, ] = c(" ", "Other", 2.82, 2.31, "17.76(15.83)", "28.45(16.83)")

apa_table.latex(as.data.frame(response.table),
                align = c(rep("l", 2), rep("c", 4)), 
                caption = "",
                note = "",
                escape = FALSE,
               col.names =  c("Cue Type", "Feature Type",
                         "Raw", "Root", "$M$ Freq. Raw", "$M$ Freq. Root"))
```

```{r, include=FALSE}
####tag examples####
##code below is from what we did last time

library(reshape)

##Creating Affix dataset
affixdata = sub1[,13:15]

##Melting dataset
affixdata$wordno = 1:nrow(affixdata)
longdata = melt(affixdata,
                id = "wordno",
                measured = c("a1", "a2", "a3"))

colnames(longdata) = c("partno", "order", "affix")

summary(longdata)

##Removing zero affixes##
longaffix = longdata
longaffix$affix[longaffix$affix == 0] = NA
summary(longaffix)
realaffix = na.omit(longaffix)
summary(realaffix)

##Actual percentages for real
final = realaffix
options(scipen = 999)
affixtable = percent(droplevels(final$affix))
#affixtable

slangwords = subset(sub1,
                    sub1$a1 == "slang")
#slangwords$feature
```

```{r, echo=FALSE, results='asis'}
####making the affix table####

affix.table = matrix(NA, nrow = 13, ncol = 3)

colnames(affix.table) = c("Affix Tag", "Example",
                         "Percent")

affix.table[1, ] = c("Actions/Processes", "ion, ment, ble, ate, ize", 7.38)
affix.table[2, ] = c("Characteristic", "y, ous, nt, ful, ive, wise", 23.55)
affix.table[3, ] = c("Location", "under, sub, mid, inter", 0.47)
affix.table[4, ] = c("Magnitude", "er, est, over, super, extra", 1.54)
affix.table[5, ] = c("Not", "less, dis, un, non, in , im, ab", 2.76)
affix.table[6, ] = c("Number", "s, uni, bi, tri, semi", 26.05)
affix.table[7, ] = c("Opposites/Wrong", "mis, anti, de", 0.15)
affix.table[8, ] = c("Past Tense", "ed", 9.11)
affix.table[9, ] = c("Person/Object", "er, or, men, person, ess, ist", 8.18)
affix.table[10, ] = c("Present Participle", "ing", 14.33)
affix.table[11, ] = c("Slang", "bros, bike, bbq, diff, h2o", 0.12)
affix.table[12, ] = c("Third Person", "s", 5.85)
affix.table[13, ] = c("Time", "fore, pre, post, re", 0.51)

apa_table.latex(as.data.frame(affix.table),
                align = c(rep("l", 2), rep("c", 1)), 
                caption = "",
                note = "",
                escape = FALSE,
               col.names =  c("Affix Tag", "Example",
                         "Percent"))
```

```{r, eval=FALSE, include=FALSE}
####fsg and bsg by cue-feature####
##getting data
tap1 = tapply(sub1$FSG,
       sub1$pos_feature, mean, na.rm = TRUE)
tap2 = tapply(sub1$FSG,
              sub1$pos_feature, sd, na.rm = TRUE)
tap1;tap2

tap3 = tapply(sub1$BSG,
       sub1$pos_feature, mean, na.rm = TRUE)
tap4 = tapply(sub1$BSG,
              sub1$pos_feature, sd, na.rm = TRUE)
tap3;tap3


tap5 = tapply(sub1$FSG,
       sub1$pos_translated, mean, na.rm = TRUE)
tap6 = tapply(sub1$FSG,
              sub1$pos_translated, sd, na.rm = TRUE)
tap5;tap6

tap7 = tapply(sub1$BSG,
       sub1$pos_translated, mean, na.rm = TRUE)
tap8 = tapply(sub1$BSG,
              sub1$pos_translated, sd, na.rm = TRUE)
tap7;tap8

```

```{r, eval=FALSE, include=FALSE}
####fsg/bsg table will go here####
##can use sub1 dataset from other tables
##Percent goes below

percentNOTmiss = function(x){ sum(is.na(x) == FALSE)/length(x) *100 }

tapply(sub1$FSG,
       sub1$pos_feature, percentNOTmiss)
tapply(sub1$BSG,
       sub1$pos_feature, percentNOTmiss)

tapply(sub1$FSG,
       sub1$pos_translated, percentNOTmiss)
tapply(sub1$BSG,
       sub1$pos_translated, percentNOTmiss)

##means and sds
tapply(sub1$FSG,
       sub1$pos_feature, mean, na.rm = TRUE)
tapply(sub1$FSG,
       sub1$pos_translated, mean, na.rm = TRUE)

tapply(sub1$FSG,
       sub1$pos_feature, sd, na.rm = TRUE)
tapply(sub1$FSG,
       sub1$pos_translated, sd, na.rm = TRUE)

mean(sub1$FSG, na.rm = TRUE)
mean(sub1$BSG, na.rm = TRUE)

sd(sub1$FSG, na.rm = TRUE)
sd(sub1$BSG, na.rm = TRUE)

##Still need total for feature and root
```

```{r, eval=FALSE, include=FALSE}
####Correlation table goes here####

#dat.cor = read.csv("acc.csv") ##for some reason it won't load the file even though i'm on the right directory

dat.cor = averaged_cosine ##imported via file viewer, acc.csv

dat.cor$jcn = as.numeric(dat.cor$jcn)
dat.cor$fsg = as.numeric(dat.cor$fsg)
dat.cor$oldcos = as.numeric(dat.cor$oldcos)
dat.cor$lsa = as.numeric(dat.cor$lsa)
dat.cor$bsg = as.numeric(dat.cor$bsg)

cor(dat.cor[ , -c(1:2)], use = "pairwise.complete.obs")

```

```{r, echo=FALSE, results='asis'}
cor.table = matrix(NA, nrow = 8, ncol = 9)

colnames(cor.table) = c(" ", "Root", "Raw", "Affix", "Previous COS", "JCN", "LSA", "FSG", "BSG")

cor.table[1, ] = c("Root", 1, " " , " " , " " , " " , " ", " ", " ")
cor.table[2, ] = c("Raw", 0.93, 1, " ", " ", " ", " ", " ", " ")
cor.table[3, ] = c("Affix", 0.50, 0.53, 1, " ", " ", " ", " ", " ")
cor.table[4, ] = c("Previous COS", 0.94, 0.91, 0.49, 1, " ", " ", " ", " ")
cor.table[5, ] = c("JCN", -0.18, -0.22, -0.17, -0.22, 1, " ", " ", " ")
cor.table[6, ] = c("LSA", 0.18, 0.15, 0.10, 0.21, -0.06, 1, " ", " ")
cor.table[7, ] = c("FSG", 0.06, 0.04, 0.08, 0.10, -0.15, 0.24, 1.00, " ")
cor.table[8, ] = c("BSG", 0.14, 0.15, 0.17, 0.18, -0.18, 0.26, 0.31, 1)

apa_table.latex(as.data.frame(cor.table),
                align = c(rep("l", 1), rep("c", 8)), 
                caption = "",
                note = "",
                escape = FALSE,
               col.names =  c(" ", "Root", "Raw", "Affix", "Previous COS", "JCN", "LSA", "FSG", "BSG"))

```

# Discussion

\newpage

# References
```{r create_r-references}
r_refs(file = "r-references.bib")
```

\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}
